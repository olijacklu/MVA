{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA Roberta Finetuning Evaluation on SuperGLUE Tasks"
      ],
      "metadata": {
        "id": "SH7Z2ugWW1ro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Necessary Libraries"
      ],
      "metadata": {
        "id": "sxnXNF3OXC93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets peft accelerate bitsandbytes evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AacjwtHIVFAM",
        "outputId": "0ab0c6a5-d01b-474a-818f-d0787cfbb6ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.2)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.5.1+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports & Setup"
      ],
      "metadata": {
        "id": "3BcEuUyLXL7F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8fkIazNUa7x"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
        "from transformers import BitsAndBytesConfig, TrainerCallback\n",
        "from evaluate import load\n",
        "from accelerate import Accelerator\n",
        "import torch\n",
        "import numpy as np\n",
        "import logging\n",
        "import time\n",
        "import os\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# THIS PARAMETER ALLOWS YOU TO CHOOSE THE SPECIFIC TASK YOU WISH TO RUN! CHANGE TO ANY OTHER SUPERGLUE TASK MENTIONED BELOW!\n",
        "# SuperGLUE tasks:\n",
        "# 'boolq', 'cb', 'copa', 'multirc', 'rte', 'wic', 'wsc', 'wsc.fixed'\n",
        "TASK = \"rte\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Specify Model"
      ],
      "metadata": {
        "id": "BZTppqoQXiZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model we chose to train from Hugging Face\n",
        "model_name = \"roberta-base\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = f\"roberta-{TASK}-finetune\""
      ],
      "metadata": {
        "id": "sFQqcOk2VJlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA Parameters"
      ],
      "metadata": {
        "id": "zC__ihwlXnnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1"
      ],
      "metadata": {
        "id": "PGn1T7iNVNUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BitsAndBytes Parameters"
      ],
      "metadata": {
        "id": "8om2QrCMXqsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False"
      ],
      "metadata": {
        "id": "bKrjJ0zvVPNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TrainingArguments Parameters"
      ],
      "metadata": {
        "id": "huUhNMJxXu4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = f\"./results_{TASK}\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 20\n",
        "\n",
        "# Enable fp16/bf16 training\n",
        "fp16 = False\n",
        "bf16 = True\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 4\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 4\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 2\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = False\n",
        "\n",
        "# Maximum gradient norm\n",
        "max_grad_norm = 1\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 1e-5\n",
        "\n",
        "# Weight decay to apply to layers\n",
        "weight_decay = 0.01\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"adamw_torch\"\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 500\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25"
      ],
      "metadata": {
        "id": "3uxyK6JkVRNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load SuperGLUE Dataset and Preprocess"
      ],
      "metadata": {
        "id": "uuIyZkoIXwhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task-specific configurations\n",
        "task_configs = {\n",
        "    'boolq': {'num_labels': 2, 'columns': ['passage', 'question'], 'label2id': {'False': 0, 'True': 1}},\n",
        "    'cb': {'num_labels': 3, 'columns': ['premise', 'hypothesis'], 'label2id': {'entailment': 0, 'contradiction': 1, 'neutral': 2}},\n",
        "    'copa': {'num_labels': 2, 'columns': ['premise', 'choice1', 'choice2'], 'special': True},\n",
        "    'multirc': {'num_labels': 2, 'columns': ['paragraph', 'question', 'answer'], 'label2id': {'False': 0, 'True': 1}},\n",
        "    'rte': {'num_labels': 2, 'columns': ['premise', 'hypothesis'], 'label2id': {'not_entailment': 0, 'entailment': 1}},\n",
        "    'wic': {'num_labels': 2, 'columns': ['sentence1', 'sentence2', 'word'], 'label2id': {'False': 0, 'True': 1}},\n",
        "    'wsc': {'num_labels': 2, 'columns': ['text', 'span1_text', 'span2_text'], 'label2id': {'False': 0, 'True': 1}},\n",
        "    'wsc.fixed': {'num_labels': 2, 'columns': ['text', 'span1_text', 'span2_text'], 'label2id': {'False': 0, 'True': 1}}\n",
        "}\n",
        "\n",
        "# Get task configuration\n",
        "task_config = task_configs.get(TASK)\n",
        "if not task_config:\n",
        "    raise ValueError(f\"Task {TASK} not supported. Choose from: {list(task_configs.keys())}\")\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"super_glue\", TASK)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
        "\n",
        "# Ensure padding\n",
        "if tokenizer.pad_token is None:\n",
        "   tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Data preprocessing function based on task type\n",
        "def get_tokenize_function(task, config):\n",
        "    \"\"\"Get the appropriate tokenize function based on the task\"\"\"\n",
        "\n",
        "    if task == 'copa':\n",
        "        def tokenize_copa(examples):\n",
        "            premises = examples['premise']\n",
        "            choices1 = examples['choice1']\n",
        "            choices2 = examples['choice2']\n",
        "            questions = examples['question']\n",
        "            labels = examples['label']\n",
        "\n",
        "            processed_examples = {\n",
        "                'input_ids': [],\n",
        "                'attention_mask': [],\n",
        "                'labels': []\n",
        "            }\n",
        "\n",
        "            for premise, choice1, choice2, question, label in zip(premises, choices1, choices2, questions, labels):\n",
        "                connector = \"because\" if question == \"cause\" else \"so\"\n",
        "\n",
        "                # Process the correct choice based on the label\n",
        "                correct_choice = choice1 if label == 0 else choice2\n",
        "                text = f\"{premise} {connector} {correct_choice}\"\n",
        "\n",
        "                encoded = tokenizer(\n",
        "                    text,\n",
        "                    return_tensors=\"np\",\n",
        "                    truncation=True,\n",
        "                    max_length=512,\n",
        "                    padding=\"max_length\"\n",
        "                )\n",
        "\n",
        "                # Add to batch\n",
        "                processed_examples['input_ids'].append(encoded['input_ids'][0])\n",
        "                processed_examples['attention_mask'].append(encoded['attention_mask'][0])\n",
        "                processed_examples['labels'].append(0)  # Always 0 since we're reformulating as binary classification\n",
        "\n",
        "            # Convert lists to numpy arrays\n",
        "            processed_examples['input_ids'] = np.array(processed_examples['input_ids'])\n",
        "            processed_examples['attention_mask'] = np.array(processed_examples['attention_mask'])\n",
        "            processed_examples['labels'] = np.array(processed_examples['labels'])\n",
        "\n",
        "            return processed_examples\n",
        "\n",
        "        return tokenize_copa\n",
        "\n",
        "    elif task == 'multirc':\n",
        "        def tokenize_multirc(examples):\n",
        "            # MultiRC needs special handling for paragraph, question, answer\n",
        "            inputs = []\n",
        "            for p, q, a in zip(examples['paragraph'], examples['question'], examples['answer']):\n",
        "                inputs.append(f\"{p} {q} {a}\")\n",
        "\n",
        "            tokenized_inputs = tokenizer(\n",
        "                inputs,\n",
        "                return_tensors=\"np\",\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                padding=\"max_length\"\n",
        "            )\n",
        "\n",
        "            tokenized_inputs[\"labels\"] = np.array(examples[\"label\"])\n",
        "            return tokenized_inputs\n",
        "\n",
        "        return tokenize_multirc\n",
        "\n",
        "    elif task == 'wic':\n",
        "        def tokenize_wic(examples):\n",
        "            # WiC needs context for word disambiguation\n",
        "            inputs = []\n",
        "            for s1, s2, word in zip(examples['sentence1'], examples['sentence2'], examples['word']):\n",
        "                inputs.append(f\"{s1} [SEP] {s2} [SEP] {word}\")\n",
        "\n",
        "            tokenized_inputs = tokenizer(\n",
        "                inputs,\n",
        "                return_tensors=\"np\",\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                padding=\"max_length\"\n",
        "            )\n",
        "\n",
        "            tokenized_inputs[\"labels\"] = np.array(examples[\"label\"])\n",
        "            return tokenized_inputs\n",
        "\n",
        "        return tokenize_wic\n",
        "\n",
        "    elif task == 'wsc' or task == 'wsc.fixed':\n",
        "        def tokenize_wsc(examples):\n",
        "            # WSC requires handling coreference resolution\n",
        "            inputs = []\n",
        "            for text, span1, span2 in zip(examples['text'], examples['span1_text'], examples['span2_text']):\n",
        "                inputs.append(f\"{text} [SEP] First span: {span1} [SEP] Second span: {span2}\")\n",
        "\n",
        "            tokenized_inputs = tokenizer(\n",
        "                inputs,\n",
        "                return_tensors=\"np\",\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                padding=\"max_length\"\n",
        "            )\n",
        "\n",
        "            tokenized_inputs[\"labels\"] = np.array(examples[\"label\"])\n",
        "            return tokenized_inputs\n",
        "\n",
        "        return tokenize_wsc\n",
        "\n",
        "    else:\n",
        "        def tokenize_default(examples):\n",
        "            if len(config['columns']) == 1:\n",
        "                tokenized_inputs = tokenizer(\n",
        "                    examples[config['columns'][0]],\n",
        "                    return_tensors=\"np\",\n",
        "                    truncation=True,\n",
        "                    max_length=512,\n",
        "                    padding=\"max_length\"\n",
        "                )\n",
        "            elif len(config['columns']) == 2:\n",
        "                tokenized_inputs = tokenizer(\n",
        "                    examples[config['columns'][0]],\n",
        "                    examples[config['columns'][1]],\n",
        "                    return_tensors=\"np\",\n",
        "                    truncation=True,\n",
        "                    max_length=512,\n",
        "                    padding=\"max_length\"\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported number of columns for task {task}\")\n",
        "\n",
        "            tokenized_inputs[\"labels\"] = np.array(examples[\"label\"])\n",
        "            return tokenized_inputs\n",
        "\n",
        "        return tokenize_default\n",
        "\n",
        "# Get the appropriate tokenize function\n",
        "tokenize_function = get_tokenize_function(TASK, task_config)\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "zrFC9c4yVVTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Evaluation Metric"
      ],
      "metadata": {
        "id": "Zu0A8MUfX4Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load appropriate metric for task\n",
        "if TASK in ['rte', 'boolq', 'wic', 'wsc', 'wsc.fixed', 'multirc']:  # Add multirc to standard accuracy\n",
        "    metric_name = 'accuracy'\n",
        "elif TASK == 'cb':\n",
        "    metric_name = 'f1'\n",
        "elif TASK == 'copa':\n",
        "    metric_name = 'accuracy'\n",
        "else:\n",
        "    metric_name = 'accuracy'\n",
        "\n",
        "metric = load(metric_name)\n",
        "\n",
        "# Function to compute metrics\n",
        "def compute_metrics(pred):\n",
        "    predictions, labels = pred\n",
        "\n",
        "    if TASK == 'cb':\n",
        "        predictions = predictions.argmax(axis=1)\n",
        "        accuracy = metric.compute(predictions=predictions, references=labels, average='weighted')\n",
        "        return accuracy\n",
        "    elif TASK == 'multirc':\n",
        "        predictions = predictions.argmax(axis=1)\n",
        "        return metric.compute(predictions=predictions, references=labels)\n",
        "    elif TASK == 'copa':\n",
        "        predictions = predictions.argmax(axis=1)\n",
        "        return {'accuracy': (predictions == labels).mean()}\n",
        "    else:\n",
        "        predictions = predictions.argmax(axis=1)\n",
        "        return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "Q9kPm8T0VeHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model & Configure LoRA"
      ],
      "metadata": {
        "id": "QRkLvjrTX8r7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get number of labels for the task\n",
        "num_labels = task_config.get('num_labels', 2)\n",
        "\n",
        "# LoRA-specific configurations\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\",\n",
        "    target_modules=[\"query\", \"key\", \"value\"]\n",
        ")\n",
        "\n",
        "# Load base model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Define label mappings based on task\n",
        "if 'label2id' in task_config:\n",
        "    label2id = task_config['label2id']\n",
        "    id2label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "    model.config.id2label = id2label\n",
        "    model.config.label2id = label2id\n",
        "\n",
        "# Add padding token\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Resize token embeddings\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Apply LoRA to the model\n",
        "lora_model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaMsJVaDVhbL",
        "outputId": "99797462-7443-43e9-ea42-8ddeb70ead90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Accelerator"
      ],
      "metadata": {
        "id": "IuL6pQKSY2Jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the accelerator\n",
        "accelerator = Accelerator(mixed_precision=\"bf16\")\n",
        "\n",
        "# Prepare the model\n",
        "lora_model = accelerator.prepare(lora_model)\n",
        "\n",
        "# Datasets and data collator are also prepared (optional, but useful for multi-GPU)\n",
        "train_dataset = accelerator.prepare(tokenized_dataset[\"train\"])\n",
        "eval_dataset = accelerator.prepare(tokenized_dataset[\"validation\"])"
      ],
      "metadata": {
        "id": "C_sB7_gKVysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training / Fine-tuning"
      ],
      "metadata": {
        "id": "KtlYztanYyrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    gradient_checkpointing=gradient_checkpointing,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=1,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_dir=f\"./logs_{TASK}\",\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    save_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# End timer\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Training Time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "# GPU memory usage\n",
        "if torch.cuda.is_available():\n",
        "    memory_used = torch.cuda.max_memory_allocated() / 1e9\n",
        "    print(f\"Maximum Memory Used: {memory_used:.2f} GB\")\n",
        "else:\n",
        "    print(\"GPU not available. Memory usage not tracked.\")\n",
        "\n",
        "# Total trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total Trainable Parameters: {trainable_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        },
        "id": "lEOr-TT2V1PL",
        "outputId": "602a83d6-5e28-441b-fc13-eda441840601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6220' max='6220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6220/6220 12:51, Epoch 19/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.329100</td>\n",
              "      <td>0.691829</td>\n",
              "      <td>0.527076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.348600</td>\n",
              "      <td>0.693211</td>\n",
              "      <td>0.498195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.353500</td>\n",
              "      <td>0.692252</td>\n",
              "      <td>0.563177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.341800</td>\n",
              "      <td>0.694466</td>\n",
              "      <td>0.472924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.354500</td>\n",
              "      <td>0.691533</td>\n",
              "      <td>0.584838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.354500</td>\n",
              "      <td>0.692690</td>\n",
              "      <td>0.487365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.326200</td>\n",
              "      <td>0.689982</td>\n",
              "      <td>0.541516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.319300</td>\n",
              "      <td>0.689714</td>\n",
              "      <td>0.595668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.350600</td>\n",
              "      <td>0.690419</td>\n",
              "      <td>0.534296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.331100</td>\n",
              "      <td>0.689108</td>\n",
              "      <td>0.570397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.338900</td>\n",
              "      <td>0.688247</td>\n",
              "      <td>0.577617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.359400</td>\n",
              "      <td>0.685596</td>\n",
              "      <td>0.602888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.350600</td>\n",
              "      <td>0.684778</td>\n",
              "      <td>0.584838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.344700</td>\n",
              "      <td>0.683213</td>\n",
              "      <td>0.592058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.329100</td>\n",
              "      <td>0.682071</td>\n",
              "      <td>0.599278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.334000</td>\n",
              "      <td>0.681535</td>\n",
              "      <td>0.606498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.344700</td>\n",
              "      <td>0.680971</td>\n",
              "      <td>0.602888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.329100</td>\n",
              "      <td>0.680759</td>\n",
              "      <td>0.613718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.695800</td>\n",
              "      <td>0.680689</td>\n",
              "      <td>0.606498</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Time: 772.27 seconds\n",
            "Maximum Memory Used: 2.92 GB\n",
            "Total Trainable Parameters: 4,131,074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save & Evaluate Fine-tuned Model"
      ],
      "metadata": {
        "id": "PJAx4KaRZhwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned LoRA model\n",
        "lora_model.save_pretrained(new_model)\n",
        "\n",
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(f\"Evaluation results for {TASK}:\")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "Fz5mRCnGV1y3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "3e0eaca6-6aaf-4a7a-981c-1598db9111d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [70/70 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results for rte:\n",
            "{'eval_loss': 0.6805618405342102, 'eval_accuracy': 0.6064981949458483, 'eval_runtime': 1.6618, 'eval_samples_per_second': 166.69, 'eval_steps_per_second': 42.124, 'epoch': 19.937399678972714}\n"
          ]
        }
      ]
    }
  ]
}