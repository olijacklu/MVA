{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfDH3TKN735l"
      },
      "source": [
        "### TP2 Advance Deep Learning\n",
        "\n",
        "**Authors:**\n",
        "- Carlos Cuevas Villarmin\n",
        "- Oliver Jack\n",
        "- Javier Alejandro Lopetegui Gonzalez\n",
        "\n",
        "*MVA, ENS Paris-Saclay*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/+MiZwhEpaX9",
        "id": "O-AOEO1Q4XTK"
      },
      "source": [
        "# **Visualization of CNNs: Grad-CAM**\n",
        "* **Objective**: Convolutional Neural Networks are widely used on computer vision. They are powerful for processing grid-like data. However we hardly know how and why they work, due to the lack of decomposability into individually intuitive components. In this assignment, we use Grad-CAM, which highlights the regions of the input image that were important for the neural network prediction.\n",
        "\n",
        "\n",
        "* NB: if `PIL` is not installed, try `conda install pillow`.\n",
        "* Computations are light enough to be done on CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "cell_ktag": "/+MiZwhEpaX9",
        "id": "aKuXra3S4XTL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Tuple\n",
        "from torchvision import models, datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import urllib.request\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/+MiZwhEpaX9",
        "id": "NY01rCJ-4XTM"
      },
      "source": [
        "## Download the Model\n",
        "We provide you with a model `DenseNet-121`, already pretrained on the `ImageNet` classification dataset.\n",
        "* **ImageNet**: A large dataset of photographs with 1 000 classes.\n",
        "* **DenseNet-121**: A deep architecture for image classification (https://arxiv.org/abs/1608.06993)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_ktag": "/+MiZwhEpaX9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUnj89Pu4XTM",
        "outputId": "80d2c61f-e546-4d6d-f9d7-3fb1a68cefc9"
      },
      "outputs": [],
      "source": [
        "densenet121 = models.densenet121(pretrained=True)\n",
        "densenet121.eval() # set the model to evaluation model\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_ktag": "/+MiZwhEpaX9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os5GJ2_v4XTM",
        "outputId": "4daf26c9-500f-44d1-e0eb-20e3188f80f3"
      },
      "outputs": [],
      "source": [
        "classes = pickle.load(urllib.request.urlopen('https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'))\n",
        "\n",
        "##classes is a dictionary with the name of each class\n",
        "print(classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/+MiZwhEpaX9",
        "id": "UEQsOogo4XTM"
      },
      "source": [
        "## Input Images\n",
        "We provide you with 20 images from ImageNet (download link on the webpage of the course or download directly using the following command line,).<br>\n",
        "In order to use the pretrained model resnet34, the input image should be normalized using `mean = [0.485, 0.456, 0.406]`, and `std = [0.229, 0.224, 0.225]`, and be resized as `(224, 224)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cell_ktag": "/+MiZwhEpaX9",
        "id": "uGk7sdI04XTM"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(dir_path):\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "    # Note: If the inverse normalisation is required, apply 1/x to the above object\n",
        "\n",
        "    dataset = datasets.ImageFolder(dir_path, transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224), # resize the image to 224x224\n",
        "            transforms.ToTensor(), # convert numpy.array to tensor\n",
        "            normalize])) #normalize the tensor\n",
        "\n",
        "    return (dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_ktag": "/+MiZwhEpaX9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2emB-8C_4XTN",
        "outputId": "a9764c54-9c94-4036-da05-ed02a52f73f8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists(\"data\"):\n",
        "    os.mkdir(\"data\")\n",
        "if not os.path.exists(\"data/TP2_images\"):\n",
        "    os.mkdir(\"data/TP2_images\")\n",
        "    !cd data/TP2_images && wget \"https://www.lri.fr/~gcharpia/deeppractice/2025/TP2/TP2_images.zip\" && unzip TP2_images.zip\n",
        "\n",
        "dir_path = \"data/\"\n",
        "dataset = preprocess_image(dir_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_ktag": "/+MiZwhEpaX9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "Qzt49HKr4XTN",
        "outputId": "ab79cb96-613a-49c6-b5a6-900609048265"
      },
      "outputs": [],
      "source": [
        "# show the orignal image\n",
        "index = 5\n",
        "input_image = Image.open(dataset.imgs[index][0]).convert('RGB')\n",
        "plt.imshow(input_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_ktag": "/+MiZwhEpaX9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEsebsIE4XTN",
        "outputId": "cda650a8-23f8-4bd8-b327-7dbb507ac5ca"
      },
      "outputs": [],
      "source": [
        "input_image = dataset[index][0].view(1, 3, 224, 224)\n",
        "output = densenet121(input_image)\n",
        "values, indices = torch.topk(output, 3)\n",
        "print(\"Top 3-classes:\", indices[0].numpy(), [classes[x] for x in indices[0].numpy()])\n",
        "print(\"Raw class scores:\", values[0].detach().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/+MiZwhEpaX9",
        "id": "7xL4Z6C-4XTN"
      },
      "source": [
        "# Grad-CAM\n",
        "* **Overview:** Given an image, and a category (‘tiger cat’) as input, we forward-propagate the image through the model to obtain the `raw class scores` before softmax. The gradients are set to zero for all classes except the desired class (tiger cat), which is set to 1. This signal is then backpropagated to the `rectified convolutional feature map` of interest, where we can compute the coarse Grad-CAM localization (blue heatmap).\n",
        "\n",
        "\n",
        "* **To Do**: Define your own function Grad_CAM to achieve the visualization of the given images. For each image, choose the top-3 possible labels as the desired classes. Compare the heatmaps of the three classes, and conclude.\n",
        "\n",
        "More precisely, you should provide a function: `show_grad_cam(image: torch.tensor) -> None` that displays something like this:\n",
        "![output_example.png](attachment:output_example.png)\n",
        "where the heatmap will be correct (here it is just an example) and the first 3 classes are the top-3 predicted classes and the last is the least probable class according to the model.\n",
        "\n",
        "* **Comment your code**: Your code should be easy to read and follow. Please comment your code, try to use the NumPy Style Python docstrings for your functions.\n",
        "\n",
        "* **To be submitted within 2 weeks**: this notebook, **cleaned** (i.e. without results, for file size reasons: `menu > kernel > restart and clean`), in a state ready to be executed (with or without GPU) (if one just presses 'Enter' till the end, one should obtain all the results for all images) with a few comments at the end. No additional report, just the notebook!\n",
        "\n",
        "\n",
        "* **Hints**:\n",
        " + We need to record the output and grad_output of the feature maps to achieve Grad-CAM. In pytorch, the function `Hook` is defined for this purpose. Read the tutorial of [hook](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) carefully.\n",
        " + More on [autograd](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html) and [hooks](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks)\n",
        " + The pretrained model densenet doesn't have an activation function after its last layer, the output is indeed the `raw class scores`, you can use them directly.\n",
        " + Your heatmap will have the same size as the feature map. You need to scale up the heatmap to the resized image (224x224, not the original one, before the normalization) for better observation purposes. The function [`torch.nn.functional.interpolate`](https://pytorch.org/docs/stable/nn.functional.html?highlight=interpolate#torch.nn.functional.interpolate) may help.  \n",
        " + Here is the link to the paper: [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/pdf/1610.02391.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/+MiZwhEpaX9",
        "id": "42jZnmiz4XTN"
      },
      "source": [
        "Class: ‘pug, pug-dog’ | Class: ‘tabby, tabby cat’\n",
        "- | -\n",
        "![alt](https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/dog.jpg)| ![alt](https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/cat.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/+MiZwhEpaX9",
        "id": "13QE_b6d4XTN"
      },
      "source": [
        "## Part 1: Grad-CAM implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "C1DT6ECK4XTN"
      },
      "outputs": [],
      "source": [
        "def grad_cam(model: nn.Module = densenet121, image: Image=None, target_layer: nn.Sequential = densenet121.features.denseblock4) -> Tuple[list,list,list]:\n",
        "    \"\"\"\n",
        "    Method to generate the Grad-CAM heatmaps given a CNN based model, an image and a target layer. It will generate it for the three classes with\n",
        "    the highest scores.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to be considered.\n",
        "        image (Image): The input image.\n",
        "        target_layer (Sequential): The target layer for Grad-CAM.\n",
        "\n",
        "    Returns:\n",
        "        all_heatmaps (list): A list of heatmaps for each class.\n",
        "        all_classes (list): A list of class indices.\n",
        "        all_scores (list): A list of scores for each class.\n",
        "    \"\"\"\n",
        "    output = model(image)\n",
        "    values, indices = torch.topk(output, k=3, dim=1)\n",
        "\n",
        "    all_heatmaps = []\n",
        "    all_classes = indices[0].tolist()\n",
        "    all_scores = values[0].tolist()\n",
        "\n",
        "    for class_idx in indices[0]:\n",
        "        model.zero_grad()\n",
        "\n",
        "        feature_maps = None\n",
        "        gradients = None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            nonlocal feature_maps\n",
        "            feature_maps = output\n",
        "\n",
        "        def backward_hook(module, grad_in, grad_out):\n",
        "            nonlocal gradients\n",
        "            gradients = grad_out[0]\n",
        "\n",
        "        forward_handle = target_layer.register_forward_hook(forward_hook)\n",
        "        backward_handle = target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        output = model(image)\n",
        "\n",
        "        one_hot = torch.zeros(output.shape, dtype=output.dtype, device=output.device)\n",
        "        one_hot[0, class_idx] = 1\n",
        "\n",
        "        output.backward(gradient=one_hot, retain_graph=True)\n",
        "\n",
        "        num_channels = feature_maps.shape[1]\n",
        "\n",
        "        alpha_k = torch.mean(gradients, dim=(2, 3))\n",
        "        alpha_k = alpha_k.view(1, num_channels, 1, 1)\n",
        "\n",
        "        grad_cam = torch.sum(alpha_k * feature_maps, dim=1)\n",
        "        grad_cam = grad_cam.squeeze(0)\n",
        "        grad_cam = F.relu(grad_cam)\n",
        "\n",
        "        heatmap = F.interpolate(\n",
        "            grad_cam.unsqueeze(0).unsqueeze(0),\n",
        "            size=(224, 224),\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        ).squeeze()\n",
        "\n",
        "        heatmap -= heatmap.min()\n",
        "        heatmap /= heatmap.max()\n",
        "\n",
        "        all_heatmaps.append(heatmap.cpu().detach())\n",
        "\n",
        "        forward_handle.remove()\n",
        "        backward_handle.remove()\n",
        "\n",
        "    return all_heatmaps, all_classes, all_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "kKwBy_lt4XTN"
      },
      "outputs": [],
      "source": [
        "def show_grad_cam(model: nn.Module = densenet121,image: Image = None, target_layer: nn.Sequential = densenet121.features.denseblock4) -> None:\n",
        "    \"\"\"\n",
        "    Method to show the Grad-CAM heatmaps given a model, an image and a target layer.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to be considered.\n",
        "        image (Image): The input image.\n",
        "        target_layer (Sequential): The target layer for Grad-CAM.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    all_heatmaps, all_classes, all_scores = grad_cam(model=model, image=image, target_layer=target_layer)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "    original_img = image.squeeze(0).permute(1, 2, 0)\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 1, 3)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 1, 3)\n",
        "    original_np = original_img * std + mean\n",
        "    original_np = original_np.clamp(0, 1).cpu().numpy()\n",
        "\n",
        "    axes[0].imshow(original_np)\n",
        "    axes[0].set_title('Input Image')\n",
        "    axes[0].set_xticks(np.arange(0, 224, 50))\n",
        "    axes[0].set_yticks(np.arange(0, 224, 50))\n",
        "\n",
        "    for i, (heatmap, class_idx, score) in enumerate(zip(all_heatmaps, all_classes, all_scores)):\n",
        "        heatmap_np = heatmap.numpy()\n",
        "\n",
        "        colormap = plt.cm.jet(heatmap_np)\n",
        "        colormap = colormap[..., :3]\n",
        "        alpha = 0.5\n",
        "        overlay = (1 - alpha) * original_np + alpha * colormap\n",
        "        overlay = np.clip(overlay, 0, 1)\n",
        "\n",
        "        im = axes[i+1].imshow(overlay)\n",
        "        class_name = classes.get(class_idx, f'Class {class_idx}')\n",
        "        axes[i+1].set_title(f'{class_name}\\nScore: {score:.3f}')\n",
        "        axes[i+1].set_xticks(np.arange(0, 224, 50))\n",
        "        axes[i+1].set_yticks(np.arange(0, 224, 50))\n",
        "\n",
        "        plt.colorbar(im, ax=axes[i+1])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/+MiZwhEpaX9",
        "id": "I6lH4Vmw4XTO"
      },
      "source": [
        "## Part 2: Try it on a few (1 to 3) images and comment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_ktag": "/+MiZwhEpaX9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "id": "rF7yuhTV4XTO",
        "outputId": "8b17839c-1eee-4871-e4df-276ddc57db94"
      },
      "outputs": [],
      "source": [
        "for index in [9, 10, 15]:\n",
        "    input_image = dataset[index][0].view(1, 3, 224, 224)\n",
        "    show_grad_cam(image=input_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zopPAlcr4XTO"
      },
      "source": [
        "Based on the obtained results, we can observe that performing Grad-Cam using the DenseNet121 model leads to good classification results in general. Overall, the heatmap seems to highlight the core areas related to the target classes. We can see that the computed score for the correct class tends to be the highest and that it distances itself clearly from the scores of the other classes. Furthermore, with the help of the heatmaps, we can see that animals with similar colour, texture or structure tend to be the next most likely classes. For example, in the case of the sorrel image, the model correctly predicts sorrel when considering the entire body of the horses, while mainly looking at the head leads to a prediction of a basenji and mainly looking at the fur leads to a prediction of an ox. Finally, the model still shows some weaknesses in some cases, as can be seen in the case of the sea lion, where the 3rd most probable class is a balance beam, largely due to the lower body part of the sea lion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/+MiZwhEpaX9",
        "id": "eMngbY754XTO"
      },
      "source": [
        "## Part 3: Try GradCAM on others convolutional layers, describe and comment the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AAT0MQkn4XTO",
        "outputId": "d07b5acf-0ba6-45db-cdb3-97cddf891abf"
      },
      "outputs": [],
      "source": [
        "target_layers = [densenet121.features.conv0,\n",
        "                 densenet121.features.denseblock1,\n",
        "                 densenet121.features.denseblock2,\n",
        "                 densenet121.features.denseblock3,\n",
        "                 densenet121.features.denseblock4]\n",
        "\n",
        "for layer in target_layers:\n",
        "    input_image = dataset[5][0].view(1, 3, 224, 224)\n",
        "    show_grad_cam(image=input_image, target_layer=layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPVKODtg4XTO"
      },
      "source": [
        "By considering different convolutional layers, we can see the evolution of how well the feature maps are able to identify the key structures of a specific class. A general observation is that as we move deeper along the architecture, the heatmap tends to localize more precisely regions that are relevant to the target class, while earlier layers tend to highlight more the contours present in the image. This progressive improvement occurs since deeper layers learn increasingly abstract and class-specific features, while shallow layers consider more basic visual elements like edges and textures that may appear throughout the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/+MiZwhEpaX9",
        "id": "eNuN_-lu4XTO"
      },
      "source": [
        "## Part 4: Try GradCAM on `9928031928.png` , describe and comment the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_ktag": "/+MiZwhEpaX9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "9LYqJ-Px4XTO",
        "outputId": "8acc1855-5299-4a49-9048-d04c24fd1848"
      },
      "outputs": [],
      "source": [
        "# elephant image without noise\n",
        "input_image = dataset[0][0].view(1, 3, 224, 224)\n",
        "show_grad_cam(image=input_image)\n",
        "\n",
        "# image 9928031928.png (adversarial example)\n",
        "input_image = dataset[-1][0].view(1, 3, 224, 224)\n",
        "show_grad_cam(image=input_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMqZIB744XTO"
      },
      "source": [
        "When comparing the two images, the human eye can easily identify an elephant in both cases. However, the model fails to predict the correct class in the second case, classifying it most likely as a jeep. Taking a closer look at the image, we can see that the colours in the image are clearly distorted compared to the original (first) image. This suggests the addition of random noise, making it an example of an adverserial attack. This shows a key vulnerability in deep neural networks, where small perturbations to the input image, not always observable to the naked eye, can cause the model to make completely wrong predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/+MiZwhEpaX9",
        "id": "MVSezOxI4XTO"
      },
      "source": [
        "## Part 5: What are the principal contributions of GradCAM (the answer is in the paper) ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DevSgawQ4XTO"
      },
      "source": [
        "Grad-CAM is introduced as a class-discriminative localization technique. The technique generates visual explanations for any CNN-based network without requiring architectural changes or re-training. It is a generalization of CAM [Zhou et al. (2026)](https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf), which is restricted to CNNs without fully-connected layers. This allows to get explainability without changing model's architechture and therefore does not compromise the performance. Grad-CAM surpasses this and can be used in CNNs with fully-connected layers (such as VGG), CNNs used for structured outputs or with multi-modal inputs.\n",
        "\n",
        "Grad-CAM is applied to classification, captioning and VQA models. It is seen to be useful for finding a reasonable explanation to predictions that originally seemed unreasonable (classification task). On the other hand, for captioning and VQA the visualizations obtained with Grad-CAM show that usual CNN with LSTM models are good localizing discriminative images regions although not being trained on grounded (image,text) pairs.\n",
        "\n",
        "The authors also show the utility of analyzing Grad-CAM visualizations in order to identify failure samples due to inductive bias in the dataset. Taking this information into consideration not only allow to gain explainability but also to obtain fair and bias-free outcomes which is very important in fields where more and more decisions are made by algorithms.\n",
        "\n",
        "Based on the neuron importance weights $\\alpha_k^c$ a deeper analysis can be done to tag each neurons to the concepts they look at in a given image. Higher positive values of the neuron importance indicate that the presence of that concept leads to an increase in the class score, whereas higher negative values indicate that its absence leads to an increase in the score for the class. Concretely, the authors use top-5 and bottom-5 concepts based on the $\\alpha_k^c$ values.\n",
        "\n",
        "To reinforce the conclusions human studies were done to show that the explanations obtained with the technique are class-discriminative. Not only helping humans to establish trust and explainability but also it was seen that even untrained users were able to differenciate between a 'stronger' and a 'weaker' network based on visualizations although they make the same prediction, i.e., the visualizations obtained with Grad-CAM are more understandable and interpretable for human being."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/+MiZwhEpaX9",
        "id": "wGVIPuTG4XTO"
      },
      "source": [
        "## Bonus 5: What are the main differences between DenseNet and ResNet ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruzByvuJ2Tgo"
      },
      "source": [
        "DenseNet and ResNet are two convolutional neural network architectures commonly used for image classification tasks. The main differences between these architectures can be summarized as follows:\n",
        "\n",
        "- **Architecture and Connectivity**\n",
        "\n",
        "ResNet introduces skip connections that allow information to bypass one or more layers. These connections facilitate the flow of gradients through the network, enabling the training of very deep models. In contrast, in DenseNet they propose a different *connectivity pattern* where each layer is directly connected to all preceding layers. This results in feature maps from all previous layers being concatenated and fed into subsequent layers.\n",
        "\n",
        "- **Feature Propagation and Reuse**\n",
        "\n",
        "The skip connections in ResNet primarily address the vanishing gradient problem, allowing for the training of extremely deep networks. DenseNet's dense connectivity pattern promotes feature reuse throughout the network, potentially leading to more efficient use of parameters.\n",
        "\n",
        "- **Model Complexity and Efficiency**\n",
        "\n",
        "ResNet typically requires more parameters due to its architecture, but it can achieve great depths, sometimes extending to hundreds or even thousands of layers. DenseNet, on the other hand, tends to be more parameter-efficient, often achieving comparable or superior performance with fewer parameters.\n",
        "\n",
        "- **Memory Usage and Computational Requirements**\n",
        "\n",
        "ResNet generally exhibits lower memory usage during training and inference. DenseNet, due to its dense connectivity and feature map concatenation, may require more memory, especially for deeper networks.\n",
        "\n",
        "- **Performance Characteristics**\n",
        "\n",
        "Both architectures have demonstrated exceptional performance across various tasks. ResNet excels in training very deep networks, while DenseNet often achieves high accuracy with fewer parameters. The choice between them typically depends on specific task requirements, available computational resources, and the nature of the dataset being used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O60G-1M4eQM"
      },
      "source": [
        "**Grad-CAM analsysis for ResNet:**\n",
        "\n",
        "After comparing the two architectures let's see the results obtained by applying Grad-CAM technique to ResNet-based models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "id": "XvG14NjUspjJ",
        "outputId": "012e38cd-1a66-48aa-eea3-e9dad9d31263"
      },
      "outputs": [],
      "source": [
        "# Load ResNet pre-trained model\n",
        "resnet18 = models.resnet18(pretrained=True)\n",
        "resnet18.eval()\n",
        "\n",
        "for index in [9, 10, 15]:\n",
        "    input_image = dataset[index][0].view(1, 3, 224, 224)\n",
        "    show_grad_cam(resnet18, image=input_image, target_layer=resnet18.layer4[1].bn2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujxEXZ2LupHw"
      },
      "source": [
        "In overall we can notice a similar behavior of the Grad-CAM despite the model we use. The main differences are in terms of model's predictions and confidence which was bigger (in the examples considered) for densenet121. Moreover, as for densenet, in the sea lion image we can see another fany example if we look at the second heatmap: the second label with highest score is *cowboy boot* and the model is looking at what should be the \"the tip of the shoe\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eai5_Z2WwKhp"
      },
      "source": [
        "**Grad-CAM evolution across layers for Resnet:**\n",
        "\n",
        "If we make the same analysis as for densenet about the evolution of Grad-CAM as we go deeper in the architecture, we can arrive to the same conclusions. Early layers appears to focus on more local features such as edges or textures while the deeper ones captures more semantic relevant information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_UfHm4Botz5E",
        "outputId": "ba31a9d3-88b9-4741-a16d-f9c806733b22"
      },
      "outputs": [],
      "source": [
        "target_layers = [resnet18.conv1,\n",
        "                 resnet18.layer1,\n",
        "                 resnet18.layer2,\n",
        "                 resnet18.layer3,\n",
        "                 resnet18.layer4]\n",
        "names = [\"conv1\", \"layer1\", \"layer2\", \"layer3\", \"layer4\"]\n",
        "input_image = dataset[5][0].view(1, 3, 224, 224)\n",
        "for i,layer in enumerate(target_layers):\n",
        "    print(f\"Applying Grad-CAM to layer: {names[i]}\")\n",
        "    show_grad_cam(model = resnet18, image=input_image, target_layer=layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PclGe_g2w4gr"
      },
      "source": [
        "**Adversarial example analysis**:\n",
        "\n",
        "In the next cell we can see the results of applying Grad-CAM in the elephant images. As we can notice, in this case the adversarial example (index=-1) does not affect significatively the model's prediction, actually the three labels with the highest scores remain the same, but in different order. It suggests that the adversarial attack was done targeting the densenet model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "QIWx9tdXt_6S",
        "outputId": "d566072e-a2db-4aa3-a1d3-6ae57ee4bf9a"
      },
      "outputs": [],
      "source": [
        "# elephant image without noise\n",
        "input_image = dataset[0][0].view(1, 3, 224, 224)\n",
        "show_grad_cam(resnet18, image=input_image, target_layer=resnet18.layer4[1].bn2)\n",
        "\n",
        "# elephant image with noise\n",
        "input_image = dataset[-1][0].view(1, 3, 224, 224)\n",
        "show_grad_cam(resnet18, image=input_image, target_layer=resnet18.layer4[1].bn2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kfiletag": "/+MiZwhEpaX9",
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
