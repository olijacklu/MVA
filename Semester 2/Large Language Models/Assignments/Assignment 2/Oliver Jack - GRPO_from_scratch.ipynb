{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Name: Oliver JACK"
      ],
      "metadata": {
        "id": "Lm4iSneWMy2v"
      },
      "id": "Lm4iSneWMy2v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TLDR: GRPO with PPO-style multiple updates per batch (similar to DeepSeek implementation)"
      ],
      "metadata": {
        "id": "VcKlXfUjMz3V"
      },
      "id": "VcKlXfUjMz3V"
    },
    {
      "cell_type": "markdown",
      "id": "80517dbc",
      "metadata": {
        "id": "80517dbc"
      },
      "source": [
        "# GRPO Training project: teach an LLM to do additions, again"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DAEbh3jBadc7",
      "metadata": {
        "id": "DAEbh3jBadc7"
      },
      "source": [
        "In this notebook, you'll find:\n",
        "* A basic Transformer with basic tokenizer\n",
        "* A basic dataset for additions\n",
        "* A classical pre-trainer, minimizing cross entropy loss\n",
        "* A Vanilla GRPO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MB8lj855LgHl",
      "metadata": {
        "id": "MB8lj855LgHl"
      },
      "source": [
        "You're not supposed to edit the existing code (you can if you want to...).\n",
        "You should implement one (or more) of the following:\n",
        "* GRPO with PPO (the `usual` one)\n",
        "* RLOO\n",
        "* ReMax\n",
        "* DPO\n",
        "* RAFT\n",
        "* your own RLHF method!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ae993bb9",
      "metadata": {
        "id": "ae993bb9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "OzGh9ahKF17h",
      "metadata": {
        "id": "OzGh9ahKF17h"
      },
      "outputs": [],
      "source": [
        "num_digits = 3\n",
        "\n",
        "dataset_size = 64_000\n",
        "train_proportion = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fabd151a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fabd151a",
        "outputId": "b6e4ad43-bd24-411c-c716-ef97d2de3255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c054bed",
      "metadata": {
        "id": "6c054bed"
      },
      "source": [
        "## Step 1: Construct a tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "t6aC9uNeIR6C",
      "metadata": {
        "id": "t6aC9uNeIR6C"
      },
      "outputs": [],
      "source": [
        "pad_token=\"[PAD]\"\n",
        "eos_token=\"[EOS]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "g2QiF-otFur3",
      "metadata": {
        "id": "g2QiF-otFur3"
      },
      "outputs": [],
      "source": [
        "class character_level_tokenizer:\n",
        "    \"\"\"\n",
        "    character-level\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.vocab = [str(x) for x in range(10)] + [\"+\", \"=\"] + [pad_token, eos_token]\n",
        "        self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n",
        "        self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n",
        "        self.ntokens = len(self.vocab)\n",
        "        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n",
        "\n",
        "    def clean(self, text):\n",
        "        \"\"\"\n",
        "        removes all characters not in the vocabulary\n",
        "        \"\"\"\n",
        "        out = re.sub(self.pattern, \"\", text)\n",
        "        return out\n",
        "\n",
        "    def pre_tokenization(self, text):\n",
        "        \"\"\"\n",
        "        character-level\n",
        "        \"\"\"\n",
        "        return [c for c in text]\n",
        "\n",
        "    def encode(self, text):\n",
        "        text_list = self.pre_tokenization(self.clean(text))\n",
        "        return [self.token_to_id[c] for c in text_list]\n",
        "\n",
        "    def decode(self, token_list):\n",
        "        return \"\".join([self.id_to_token[x] for x in token_list])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "QuCc6jF5F8hK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuCc6jF5F8hK",
        "outputId": "c8764fe4-c3a4-454a-e3d6-5af1f1ddc22c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tokenizer = character_level_tokenizer()\n",
        "ntokens = tokenizer.ntokens\n",
        "ntokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8FXW2K-1Jd-P",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FXW2K-1Jd-P",
        "outputId": "4e747804-9245-42fc-9945-5b0330003ec8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1, 2, 10, 4, 2, 11], '12+42=')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "prompt = \"12 + 42 =\"\n",
        "inputs = tokenizer.encode(prompt)\n",
        "inputs, tokenizer.decode(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "491af297",
      "metadata": {
        "id": "491af297"
      },
      "source": [
        "## Step 2: Create a dataset for arithmetic operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "daa90f31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daa90f31",
        "outputId": "2feaf3d0-396c-4eec-bd5a-cae4a970ba64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('386+039=', '425')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "def sample_datapoint(num_digits = 3):\n",
        "    a_list = [random.randint(0, 9) for _ in range(num_digits)]\n",
        "    b_list = [random.randint(0, 9) for _ in range(num_digits)]\n",
        "    a_int = int(\"\".join([str(x) for x in a_list]))\n",
        "    b_int = int(\"\".join([str(x) for x in b_list]))\n",
        "    a_str = \"\".join([str(x) for x in a_list])\n",
        "    b_str = \"\".join([str(x) for x in b_list])\n",
        "    sum_int = a_int + b_int\n",
        "    return (a_str + \"+\" + b_str + \"=\", str(sum_int))\n",
        "\n",
        "sample_datapoint(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b6e861d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6e861d2",
        "outputId": "f8886449-1ab4-4dd0-f237-eba3f3c55a81"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('891+316=', '1207'),\n",
              " ('747+075=', '822'),\n",
              " ('358+301=', '659'),\n",
              " ('213+636=', '849')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "data = []\n",
        "for _ in range(dataset_size):\n",
        "    data.append(sample_datapoint(num_digits))\n",
        "data[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fee85050",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fee85050",
        "outputId": "3c7aa9e5-e102-4bd6-88a9-67472690a8c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(57600, 6400)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "data_train = data[: int(train_proportion * dataset_size)]\n",
        "data_test = data[int(train_proportion * dataset_size):]\n",
        "\n",
        "len(data_train),len(data_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37200598",
      "metadata": {
        "id": "37200598"
      },
      "source": [
        "## Step 3: Construct a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "91674239",
      "metadata": {
        "id": "91674239"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
        "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
        "        Here, we use sine and cosine functions of different frequencies.\n",
        "    .. math:\n",
        "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "        \\text{where pos is the word position and i is the embed idx)\n",
        "    Args:\n",
        "        d_model: the embed dim (required).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        max_len: the max. length of the incoming sequence (default=5000).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            x: the sequence fed to the positional encoder model (required).\n",
        "        Shape:\n",
        "            x: [sequence length, batch size, embed dim]\n",
        "            output: [sequence length, batch size, embed dim]\n",
        "        \"\"\"\n",
        "\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4eb278ab",
      "metadata": {
        "id": "4eb278ab"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Transformer):\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__(d_model=ninp,\n",
        "                                               nhead=nhead,\n",
        "                                               dim_feedforward=nhid,\n",
        "                                               num_encoder_layers=nlayers)\n",
        "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.ninp = ninp\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        return torch.log(torch.tril(torch.ones(sz,sz)))\n",
        "\n",
        "    def forward(self, src):\n",
        "        mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "        self.src_mask = mask\n",
        "\n",
        "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output_enc = self.encoder(src, mask=self.src_mask)\n",
        "        output_dec = self.decoder(output_enc)\n",
        "        return output_dec, output_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1d568cc4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d568cc4",
        "outputId": "b55ee262-dd1d-4be5-fbbd-892758c1c5c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerModel(\n",
              "  (encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-7): 8 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (decoder): Linear(in_features=128, out_features=14, bias=True)\n",
              "  (input_emb): Embedding(14, 128)\n",
              "  (pos_encoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "model = TransformerModel(ntoken = ntokens,\n",
        "                         ninp = 128,\n",
        "                         nhead = 16,\n",
        "                         nhid = 64,\n",
        "                         nlayers = 8)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a6PmJSo95N4C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6PmJSo95N4C",
        "outputId": "4eae06ad-e91a-40da-92a2-db3c290b591c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 668942\n"
          ]
        }
      ],
      "source": [
        "print(\"number of parameters: {}\".format(sum([x.numel() for x in model.parameters()])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e35d113",
      "metadata": {
        "id": "2e35d113"
      },
      "source": [
        "### Useful functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8f2f06e0",
      "metadata": {
        "id": "8f2f06e0"
      },
      "outputs": [],
      "source": [
        "def generate(model, prompts, new_tokens = 5, mode = \"greedy\", num_samples = 1, temperature = 0.8):\n",
        "    input_tensor = torch.repeat_interleave(prompts, repeats = num_samples, dim = 1).to(device)\n",
        "    # (prompt_length, batch_size * num_samples)\n",
        "    for _ in range(new_tokens):\n",
        "        output, _ = model(input_tensor) # (prompt_length, batch_size * num_samples, ntokens)\n",
        "        logits = output[-1,:,:] # (batch_size * num_samples, ntokens)\n",
        "        if mode == \"greedy\":\n",
        "            tokens = torch.argmax(logits, -1).view((1,-1)) # (1, batch_size * num_samples)\n",
        "        else: # mode == \"sampling\"\n",
        "            logits /= temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            tokens = torch.multinomial(probs, num_samples = 1).view((1,-1)) # (1, batch_size * num_samples)\n",
        "        input_tensor = torch.cat((input_tensor, tokens), 0)\n",
        "    return input_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d76d1b19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d76d1b19",
        "outputId": "b0a7dff3-7c17-4428-f8ef-ff1e736eb3f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 2, 10,  3, 11,  7, 13,  7, 13,  7]], device='cuda:0'),\n",
              " '2+3=7[EOS]7[EOS]7')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "prompt = \"2+3=\"\n",
        "prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
        "output = generate(model, prompt_tensor).view((1,-1))\n",
        "output, tokenizer.decode(output[0].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "00954ddc",
      "metadata": {
        "id": "00954ddc"
      },
      "outputs": [],
      "source": [
        "def pad(token_list, type_list = \"prompts\"):\n",
        "    max_length = max([len(x) for x in token_list])\n",
        "    out = []\n",
        "    for x in token_list:\n",
        "        if type_list == \"prompts\":\n",
        "            out.append([tokenizer.token_to_id[pad_token]] * (max_length - len(x)) + x)\n",
        "        if type_list == \"answers\":\n",
        "            out.append(x + [tokenizer.token_to_id[eos_token]] + [tokenizer.token_to_id[pad_token]] * (max_length - len(x)))\n",
        "    return out, max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2c84beab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c84beab",
        "outputId": "1d26616f-2703-426c-d411-5a145eee7797"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['[PAD][PAD]1+1=', '21+35='], ['2[EOS][PAD]', '56[EOS]'])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "prompts = [tokenizer.encode(\"1+1=\"), tokenizer.encode(\"21+35=\")]\n",
        "answers = [tokenizer.encode(\"2\"), tokenizer.encode(\"56\")]\n",
        "padded_prompts, _ = pad(prompts, \"prompts\")\n",
        "padded_answers, _ = pad(answers, \"answers\")\n",
        "padded_prompts, padded_answers\n",
        "[tokenizer.decode(p) for p in padded_prompts], [tokenizer.decode(p) for p in padded_answers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "264f9227",
      "metadata": {
        "id": "264f9227"
      },
      "outputs": [],
      "source": [
        "def get_batch(split, i, batch_size):\n",
        "    data = data_train if split == 'train' else data_test\n",
        "\n",
        "    prompts = [data[i][0] for i in range(i, i + batch_size)]\n",
        "    encoded_prompts = [tokenizer.encode(prompt) for prompt in prompts]\n",
        "    padded_prompts, prompt_length = pad(encoded_prompts, \"prompts\")\n",
        "\n",
        "    answers = [data[i][1] for i in range(i, i + batch_size)]\n",
        "    encoded_answers = [tokenizer.encode(answer) for answer in answers]\n",
        "    padded_answers, answers_length = pad(encoded_answers, \"answers\")\n",
        "\n",
        "    X = torch.stack([torch.tensor(x) for x in padded_prompts], 1)\n",
        "    Y = torch.stack([torch.tensor(x) for x in padded_answers], 1)\n",
        "    return X, Y, prompt_length, answers_length, prompts, answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "91e281ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91e281ad",
        "outputId": "498651f4-1529-49e6-d5f1-89621910abfd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([8, 16]), torch.Size([5, 16]), 8, 4, '575+346=', '921')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "X, Y, prompt_length, answers_length, prompts, answers = get_batch(\"train\", 43, 16)\n",
        "X.shape, Y.shape, prompt_length, answers_length, prompts[0], answers[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "113e1fd1",
      "metadata": {
        "id": "113e1fd1"
      },
      "source": [
        "## Step 4: Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "KfmcSdPwp3K6",
      "metadata": {
        "id": "KfmcSdPwp3K6"
      },
      "outputs": [],
      "source": [
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "1cfcd10a",
      "metadata": {
        "id": "1cfcd10a"
      },
      "outputs": [],
      "source": [
        "def evaluate(batch_size = batch_size):\n",
        "    # Turn on evaluation mode disables dropout.\n",
        "    model.eval()\n",
        "    correct = 0.\n",
        "    with torch.no_grad():\n",
        "        for batch, i in enumerate(range(0, len(data_test) - 1, batch_size)):\n",
        "            prompts, target_answers, prompt_length, answers_length, _, _ = get_batch(\"test\", i, batch_size)\n",
        "            prompts = prompts.to(device) # (prompt_length, batch_size)\n",
        "            target_answers = target_answers.to(device) # (answers_length + 1, batch_size)\n",
        "            output = generate(model, prompts, answers_length + 1) # (prompt_length + answers_length + 1, batch_size)\n",
        "            answers_tokens = output[prompt_length:, :] # (answers_length + 1, batch_size), contains tokens\n",
        "            equality_test = answers_tokens == target_answers # (answers_length + 1, batch_size), contains boolean values\n",
        "            correct += torch.all(equality_test, axis=0).float().sum()\n",
        "        accuracy = correct / len(data_test)\n",
        "    return accuracy.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ac335b05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac335b05",
        "outputId": "04e776b6-20dc-4c20-8e08-7e2c8c42aa06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c54061a",
      "metadata": {
        "id": "4c54061a"
      },
      "source": [
        "## Step 5: Train the model, classical approach"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b827e567",
      "metadata": {
        "id": "b827e567"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5b140ba3",
      "metadata": {
        "id": "5b140ba3"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "batch_size = 16\n",
        "learning_rate = 8e-4\n",
        "\n",
        "reporting_per_epoch = 5\n",
        "log_interval = len(data_train) // (reporting_per_epoch + 1)\n",
        "assert(log_interval % batch_size == 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "3638a75d",
      "metadata": {
        "id": "3638a75d"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    best_test_accuracy = None\n",
        "    test_accuracy = evaluate()\n",
        "    print('-' * 89)\n",
        "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
        "    print('-' * 89)\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss = 0.\n",
        "        start_time = time.time()\n",
        "        for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
        "            prompts, target_answers, prompt_length, answers_length, _, _ = get_batch(\"train\", i, batch_size)\n",
        "            prompts = prompts.to(device) # (prompt_length, batch_size)\n",
        "            target_answers = target_answers.to(device) # (answers_length + 1, batch_size)\n",
        "            input_tensor = torch.cat((prompts, target_answers), 0) # (prompt_length + answers_length + 1, batch_size)\n",
        "            model.zero_grad()\n",
        "            output, _ = model(input_tensor) # (prompt_length + answers_length + 1, batch_size, ntokens)\n",
        "            output_answers = output[prompt_length-1:-1,:,:].reshape(-1, ntokens) # ((answers_length + 1) * batch_size, ntokens)\n",
        "            target_answers = target_answers.view(-1)\n",
        "            loss = F.cross_entropy(output_answers, target_answers)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if i % log_interval == 0 and batch > 0:\n",
        "                cur_loss = total_loss / log_interval\n",
        "                elapsed = time.time() - start_time\n",
        "                print('| {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f} | perplexity {:8.2f}'.format(batch, len(data_train) // batch_size,\n",
        "                                                                                                            elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "                total_loss = 0\n",
        "                start_time = time.time()\n",
        "        test_accuracy = evaluate()\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the test accuracy is the best we've seen so far.\n",
        "        if not best_test_accuracy or test_accuracy > best_test_accuracy:\n",
        "            with open(\"arithmetic_v1.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_test_accuracy = test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4e2a8490",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e2a8490",
        "outputId": "ae30b1d8-1e53-4f8a-ab10-72cb70500325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| initialisation | test accuracy  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  1.11 | loss  0.09 | perplexity     1.09\n",
            "|  1200/ 3600 batches | ms/batch  1.09 | loss  0.07 | perplexity     1.07\n",
            "|  1800/ 3600 batches | ms/batch  1.07 | loss  0.07 | perplexity     1.07\n",
            "|  2400/ 3600 batches | ms/batch  1.07 | loss  0.07 | perplexity     1.07\n",
            "|  3000/ 3600 batches | ms/batch  1.07 | loss  0.07 | perplexity     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 72.07s | test accuracy  0.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  1.07 | loss  0.06 | perplexity     1.07\n",
            "|  1200/ 3600 batches | ms/batch  1.07 | loss  0.06 | perplexity     1.07\n",
            "|  1800/ 3600 batches | ms/batch  1.07 | loss  0.05 | perplexity     1.05\n",
            "|  2400/ 3600 batches | ms/batch  1.07 | loss  0.04 | perplexity     1.04\n",
            "|  3000/ 3600 batches | ms/batch  1.07 | loss  0.04 | perplexity     1.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 71.30s | test accuracy  0.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  1.07 | loss  0.04 | perplexity     1.04\n",
            "|  1200/ 3600 batches | ms/batch  1.08 | loss  0.04 | perplexity     1.04\n",
            "|  1800/ 3600 batches | ms/batch  1.07 | loss  0.03 | perplexity     1.03\n",
            "|  2400/ 3600 batches | ms/batch  1.08 | loss  0.03 | perplexity     1.04\n",
            "|  3000/ 3600 batches | ms/batch  1.07 | loss  0.04 | perplexity     1.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 71.72s | test accuracy  0.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  1.08 | loss  0.03 | perplexity     1.03\n",
            "|  1200/ 3600 batches | ms/batch  1.07 | loss  0.03 | perplexity     1.03\n",
            "|  1800/ 3600 batches | ms/batch  1.07 | loss  0.03 | perplexity     1.03\n",
            "|  2400/ 3600 batches | ms/batch  1.08 | loss  0.02 | perplexity     1.02\n",
            "|  3000/ 3600 batches | ms/batch  1.07 | loss  0.02 | perplexity     1.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 71.70s | test accuracy  0.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  1.07 | loss  0.02 | perplexity     1.02\n",
            "|  1200/ 3600 batches | ms/batch  1.07 | loss  0.01 | perplexity     1.01\n",
            "|  1800/ 3600 batches | ms/batch  1.07 | loss  0.01 | perplexity     1.01\n",
            "|  2400/ 3600 batches | ms/batch  1.07 | loss  0.01 | perplexity     1.01\n",
            "|  3000/ 3600 batches | ms/batch  1.07 | loss  0.01 | perplexity     1.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 71.32s | test accuracy  0.79\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "56d9d440",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56d9d440",
        "outputId": "f5fb283c-29cb-45b7-8af4-931ba3094e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "533+955=1488[EOS]\t actual result: 1488\n",
            "969+150=1118[EOS]\t actual result: 1119\n",
            "876+729=1605[EOS]\t actual result: 1605\n",
            "376+353=729[EOS]\t actual result: 729\n",
            "903+755=1658[EOS]\t actual result: 1658\n",
            "452+736=1188[EOS]\t actual result: 1188\n",
            "153+522=675[EOS]\t actual result: 675\n",
            "663+621=1284[EOS]\t actual result: 1284\n",
            "089+361=450[EOS]\t actual result: 450\n",
            "706+614=1320[EOS]\t actual result: 1320\n",
            "245+176=421[EOS]\t actual result: 421\n",
            "659+747=1406[EOS]\t actual result: 1406\n",
            "969+305=1274[EOS]\t actual result: 1274\n",
            "314+784=1097[EOS]\t actual result: 1098\n",
            "994+280=1274[EOS]\t actual result: 1274\n",
            "049+609=657[EOS]\t actual result: 658\n",
            "801+751=1552[EOS]\t actual result: 1552\n",
            "615+012=626[EOS]\t actual result: 627\n",
            "844+221=1064[EOS]\t actual result: 1065\n",
            "596+812=1408[EOS]\t actual result: 1408\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "for i in range(20):\n",
        "    prompt, answers = data_test[i]\n",
        "    prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
        "    output = generate(model, prompt_tensor, len(answers) + 1).view((1,-1))\n",
        "    print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfa4c591",
      "metadata": {
        "id": "cfa4c591"
      },
      "source": [
        "## Step 4 bis: Vanilla GRPO training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aff83f72",
      "metadata": {
        "id": "aff83f72"
      },
      "source": [
        "### Custom reward functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "3c548bf7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c548bf7",
        "outputId": "a7e43384-2e92-4731-f730-7a8ac4f2f255"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "def accuracy_reward(output, answer):\n",
        "    pattern = r\"\\[EOS\\]\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    pattern = r\"(\\[PAD\\])*$\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    return 1. if output == answer else 0.\n",
        "\n",
        "accuracy_reward(\"123[EOS][PAD][PAD]\", \"123\"), accuracy_reward(\"123\", \"124\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "e1f02762",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1f02762",
        "scrolled": true,
        "outputId": "27745f99-8f5e-4b01-bbc2-0fd0556988da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 0.9919354838709677)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "def distance_accuracy_reward(output, answer):\n",
        "    pattern = r\"\\[EOS\\]\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    pattern = r\"(\\[PAD\\])*$\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    int_output = int(output)\n",
        "    int_answer = int(answer)\n",
        "    return 1 - abs(int_output - int_answer) / max(int_output, int_answer)\n",
        "\n",
        "distance_accuracy_reward(\"123[EOS]\", \"123\"), distance_accuracy_reward(\"123[PAD]\", \"124\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b42a0d70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b42a0d70",
        "outputId": "92e0a263-afbf-4cd1-ad33-43af1c9a9e7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "def digit_accuracy_reward(output, answer):\n",
        "    pattern = r\"\\[EOS\\]\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    pattern = r\"(\\[PAD\\])*$\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    return sum(c1 == c2 for (c1,c2) in zip(output, answer)) / max(len(output), len(answer))\n",
        "\n",
        "digit_accuracy_reward(\"123[EOS][PAD][PAD]\", \"123\"), digit_accuracy_reward(\"123[EOS]\", \"123\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "a41603b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a41603b2",
        "outputId": "517c70aa-69a5-4dae-cac8-7be2cde759dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 1.0, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "def reward_format(output):\n",
        "    pattern = r\"\\d+\\[EOS\\](\\[PAD\\])*$\"\n",
        "    return 1. if bool(re.match(pattern, output)) else 0.\n",
        "\n",
        "reward_format(\"123[EOS][PAD][PAD]\"), reward_format(\"123[EOS]\"), reward_format(\"123\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4482e411",
      "metadata": {
        "id": "4482e411"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "cf764cb0",
      "metadata": {
        "id": "cf764cb0"
      },
      "outputs": [],
      "source": [
        "epochs = 3\n",
        "batch_size = 16\n",
        "learning_rate = 1e-4\n",
        "num_samples = 16\n",
        "temperature = .8\n",
        "\n",
        "beta = 0.04\n",
        "m = 5\n",
        "epsilon = 0.2\n",
        "\n",
        "reporting_per_epoch = 5\n",
        "log_interval = len(data_train) // (reporting_per_epoch + 1)\n",
        "assert(log_interval % batch_size == 0)\n",
        "\n",
        "reward_fun = digit_accuracy_reward\n",
        "reward_format = reward_format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "3vrflshEX9UV",
      "metadata": {
        "id": "3vrflshEX9UV"
      },
      "outputs": [],
      "source": [
        "def compute_rewards(text_outputs, answers):\n",
        "    repeated_answers = [answer for answer in answers for _ in range(num_samples)]\n",
        "    rewards = torch.tensor(\n",
        "        [0.2 * reward_format(output) + 0.8 * reward_fun(output, answer)\n",
        "         for output, answer in zip(text_outputs, repeated_answers)],\n",
        "        dtype=torch.float32,\n",
        "        device=device\n",
        "    )\n",
        "    return rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "yUj6n0lUYDlY",
      "metadata": {
        "id": "yUj6n0lUYDlY"
      },
      "outputs": [],
      "source": [
        "def calculate_grpo_advantages(rewards):\n",
        "    # Reshape rewards to have shape [batch_size, num_samples]\n",
        "    grouped_rewards = rewards.view(-1, num_samples)\n",
        "\n",
        "    # Calculate mean and std within each group\n",
        "    mean_rewards = grouped_rewards.mean(dim=1, keepdim=True)\n",
        "    std_rewards = grouped_rewards.std(dim=1, keepdim=True) + 1e-8\n",
        "\n",
        "    # Normalize rewards within each group\n",
        "    normalized_rewards = (grouped_rewards - mean_rewards) / std_rewards\n",
        "\n",
        "    # Reshape back to original shape\n",
        "    advantages = normalized_rewards.view(-1)\n",
        "\n",
        "    return advantages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "edqc_fAIZaV0",
      "metadata": {
        "id": "edqc_fAIZaV0"
      },
      "outputs": [],
      "source": [
        "def compute_log_probs(model, outputs, prompt_length):\n",
        "    logits, _ = model(outputs)\n",
        "    # logits.shape = (prompt_length + answers_length + 1, batch_size * num_samples, vocab_size)\n",
        "\n",
        "    # we only need the log probabilities for the new tokens\n",
        "    # this introduces a shift: the logits for a position are the predictions for the next token\n",
        "    logits = logits[prompt_length-1:-1, :, :]\n",
        "    # logits.shape = (answers_length + 1, batch_size * num_samples, vocab_size)\n",
        "\n",
        "    # convert raw logits into log probabilities along the vocabulary axis\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    # log_probs.shape = (answers_length + 1, batch_size * num_samples, vocab_size)\n",
        "    return log_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "q5RiA9NBu7BS",
      "metadata": {
        "id": "q5RiA9NBu7BS"
      },
      "outputs": [],
      "source": [
        "def get_selected_logprobs(log_probs, responses):\n",
        "    # Add dimension for gathering\n",
        "    responses = responses.unsqueeze(-1)\n",
        "\n",
        "    # Gather log probabilities for the tokens that were actually chosen\n",
        "    selected_log_probs = log_probs.gather(dim=-1, index=responses)\n",
        "    selected_log_probs = selected_log_probs.squeeze(-1)\n",
        "\n",
        "    return selected_log_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "cqWm5e9bYHyH",
      "metadata": {
        "id": "cqWm5e9bYHyH"
      },
      "outputs": [],
      "source": [
        "def compute_ppo_grpo_loss(logprobs_current, logprobs_ref, advantages, epsilon, num_samples, logprobs_old=None, beta=0.04):\n",
        "    # Use current logprobs as old logprobs if not provided\n",
        "    if logprobs_old is None:\n",
        "        old_logprobs = logprobs_current.detach()\n",
        "    else:\n",
        "        old_logprobs = logprobs_old\n",
        "\n",
        "    # Calculate KL divergence between current and reference model\n",
        "    if beta > 0:\n",
        "        per_token_kl = (torch.exp(logprobs_ref.detach() - logprobs_current) - (logprobs_ref.detach() - logprobs_current) - 1)\n",
        "\n",
        "    # Calculate PPO clipping objective\n",
        "    ratio = torch.exp(logprobs_current - old_logprobs)\n",
        "    clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
        "\n",
        "    surrogate1 = ratio * advantages.unsqueeze(0)\n",
        "    surrogate2 = clipped_ratio * advantages.unsqueeze(0)\n",
        "\n",
        "    # Take minimum of surrogate terms\n",
        "    per_token_loss = -torch.min(surrogate1, surrogate2)\n",
        "\n",
        "    # Add KL penalty if beta > 0\n",
        "    if beta > 0:\n",
        "        per_token_loss = per_token_loss + beta * per_token_kl\n",
        "\n",
        "    # Average over tokens and samples\n",
        "    loss = per_token_loss.mean()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "mSpxHF0Gxu3h",
      "metadata": {
        "id": "mSpxHF0Gxu3h"
      },
      "outputs": [],
      "source": [
        "def train_PPO_GRPO(verbose=False):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    best_test_accuracy = None\n",
        "    test_accuracy = evaluate()\n",
        "    print('-' * 89)\n",
        "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
        "    print('-' * 89)\n",
        "\n",
        "    # switch eval for train model (enables dropout)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Create/update reference model at the beginning of each epoch\n",
        "        reference_model = copy.deepcopy(model)\n",
        "        reference_model.eval()\n",
        "\n",
        "        for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
        "\n",
        "            # get a batch of prompts and answers\n",
        "            prompts, _, prompt_length, answers_length, questions, answers = get_batch(\"train\", i, batch_size)\n",
        "            prompts = prompts.to(device)\n",
        "\n",
        "            # generate samples for each prompt using current model\n",
        "            outputs = generate(model,\n",
        "                              prompts,\n",
        "                              new_tokens=answers_length + 1,\n",
        "                              mode=\"sampling\",\n",
        "                              num_samples=num_samples,\n",
        "                              temperature=temperature)\n",
        "\n",
        "            # Get responses\n",
        "            responses = outputs[prompt_length:, :]\n",
        "\n",
        "            # Decode outputs for reward calculation\n",
        "            text_outputs = [tokenizer.decode(outputs[prompt_length:, i].tolist()) for i in range(outputs.size(1))]\n",
        "\n",
        "            # Calculate rewards and advantages\n",
        "            rewards = compute_rewards(text_outputs, answers)\n",
        "            advantages = calculate_grpo_advantages(rewards)\n",
        "\n",
        "            # Get reference model log probabilities\n",
        "            with torch.no_grad():\n",
        "                log_probs_ref = compute_log_probs(reference_model, outputs, prompt_length)\n",
        "                selected_log_probs_ref = get_selected_logprobs(log_probs_ref, responses)\n",
        "\n",
        "            # Multiple PPO updates per batch\n",
        "            selected_log_probs_old = None\n",
        "            for update_idx in range(m):\n",
        "                # Compute current model log probabilities\n",
        "                log_probs_current = compute_log_probs(model, outputs, prompt_length)\n",
        "                selected_log_probs_current = get_selected_logprobs(log_probs_current, responses)\n",
        "\n",
        "                # Compute PPO GRPO loss\n",
        "                loss = compute_ppo_grpo_loss(\n",
        "                    selected_log_probs_current,\n",
        "                    selected_log_probs_ref,\n",
        "                    advantages,\n",
        "                    epsilon,\n",
        "                    num_samples,\n",
        "                    logprobs_old=selected_log_probs_old,\n",
        "                    beta=beta\n",
        "                )\n",
        "\n",
        "                # Optimize\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update old log probs for next iteration\n",
        "                selected_log_probs_old = selected_log_probs_current.detach()\n",
        "\n",
        "            if i % log_interval == 0 and batch > 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print('| {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f}'.format(\n",
        "                    batch, len(data_train) // batch_size, elapsed * 1000 / log_interval, loss.item()))\n",
        "\n",
        "                if verbose:\n",
        "                    print(\"\\nquestion:\", questions[0],\n",
        "                          \"\\nanswer:\", answers[0],\n",
        "                          \"\\noutput:\", text_outputs[:num_samples],\n",
        "                          \"\\nreward:\", rewards[:num_samples],\n",
        "                          \"\\nadvantage:\", advantages[:num_samples], \"\\n\")\n",
        "\n",
        "                start_time = time.time()\n",
        "\n",
        "        test_accuracy = evaluate()\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(\n",
        "            epoch, (time.time() - epoch_start_time), test_accuracy))\n",
        "        print('-' * 89)\n",
        "\n",
        "        # Save the model if the test accuracy is the best we've seen so far\n",
        "        if not best_test_accuracy or test_accuracy > best_test_accuracy:\n",
        "            with open(\"arithmetic_PPO_GRPO.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_test_accuracy = test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "b02716ff",
      "metadata": {
        "id": "b02716ff",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cde7bd7-dc6a-4b13-8ffe-933501dc33f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| initialisation | test accuracy  0.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  9.58 | loss 41.24\n",
            "|  1200/ 3600 batches | ms/batch  9.50 | loss 93.90\n",
            "|  1800/ 3600 batches | ms/batch  9.49 | loss  1.83\n",
            "|  2400/ 3600 batches | ms/batch  9.53 | loss  0.91\n",
            "|  3000/ 3600 batches | ms/batch  9.51 | loss  0.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 558.57s | test accuracy  0.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  8.90 | loss  0.03\n",
            "|  1200/ 3600 batches | ms/batch  8.85 | loss  0.02\n",
            "|  1800/ 3600 batches | ms/batch  8.86 | loss  0.02\n",
            "|  2400/ 3600 batches | ms/batch  8.89 | loss  0.02\n",
            "|  3000/ 3600 batches | ms/batch  8.86 | loss  0.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 520.90s | test accuracy  1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  8.88 | loss  0.00\n",
            "|  1200/ 3600 batches | ms/batch  8.87 | loss  0.00\n",
            "|  1800/ 3600 batches | ms/batch  8.86 | loss  0.00\n",
            "|  2400/ 3600 batches | ms/batch  8.84 | loss  0.00\n",
            "|  3000/ 3600 batches | ms/batch  8.85 | loss  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 519.90s | test accuracy  1.00\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "train_PPO_GRPO(verbose = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "aeVn935w5BSp",
      "metadata": {
        "id": "aeVn935w5BSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18019864-f3e9-4c55-914e-5e5d9f16891e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "533+955=1488[EOS]\t actual result: 1488\n",
            "969+150=1119[EOS]\t actual result: 1119\n",
            "876+729=1605[EOS]\t actual result: 1605\n",
            "376+353=729[EOS]\t actual result: 729\n",
            "903+755=1658[EOS]\t actual result: 1658\n",
            "452+736=1188[EOS]\t actual result: 1188\n",
            "153+522=675[EOS]\t actual result: 675\n",
            "663+621=1284[EOS]\t actual result: 1284\n",
            "089+361=450[EOS]\t actual result: 450\n",
            "706+614=1320[EOS]\t actual result: 1320\n",
            "245+176=421[EOS]\t actual result: 421\n",
            "659+747=1406[EOS]\t actual result: 1406\n",
            "969+305=1274[EOS]\t actual result: 1274\n",
            "314+784=1098[EOS]\t actual result: 1098\n",
            "994+280=1274[EOS]\t actual result: 1274\n",
            "049+609=658[EOS]\t actual result: 658\n",
            "801+751=1552[EOS]\t actual result: 1552\n",
            "615+012=627[EOS]\t actual result: 627\n",
            "844+221=1065[EOS]\t actual result: 1065\n",
            "596+812=1408[EOS]\t actual result: 1408\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "for i in range(20):\n",
        "    prompt, answers = data_test[i]\n",
        "    prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
        "    output = generate(model, prompt_tensor, len(answers) + 1).view((1,-1))\n",
        "    print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}