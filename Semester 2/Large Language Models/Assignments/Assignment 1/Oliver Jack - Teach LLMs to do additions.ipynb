{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80517dbc",
   "metadata": {
    "id": "80517dbc"
   },
   "source": [
    "# Teach an LLM to do additions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d116b3",
   "metadata": {},
   "source": [
    "## Name: Oliver Jack, Mail: olijacklu@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186d6eb7",
   "metadata": {},
   "source": [
    "## TLDR: Used aligning numbers & reversing for tokenizer (123+456 -> [(3,6), (2,5), (1,4)]) and Abacus approach for positional embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaca18f",
   "metadata": {},
   "source": [
    "The goal of this project is to teach an LLM to do additions, playing only with two parts:\n",
    "* the tokenizer\n",
    "* the positional embedding\n",
    "\n",
    "Both the model and the dataset are fixed.\n",
    "\n",
    "You are allowed to tune the hyperparameters, but this is not the main goal. Depending on the quality of your tokenizer and positional embedding, you may change the number of bits. The initial value of 3 is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae993bb9",
   "metadata": {
    "id": "ae993bb9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "OzGh9ahKF17h",
   "metadata": {
    "id": "OzGh9ahKF17h"
   },
   "outputs": [],
   "source": [
    "number_bits = 12\n",
    "\n",
    "dataset_size = 64_000\n",
    "train_proportion = 0.9\n",
    "\n",
    "log_interval = 200\n",
    "batch_size = 64\n",
    "epochs = 4\n",
    "learning_rate = 8e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c054bed",
   "metadata": {
    "id": "6c054bed"
   },
   "source": [
    "## Step 1: Construct a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "t6aC9uNeIR6C",
   "metadata": {
    "id": "t6aC9uNeIR6C"
   },
   "outputs": [],
   "source": [
    "pad_token=\"[PAD]\"\n",
    "eos_token=\"[EOS]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BMvT0B-MGBnY",
   "metadata": {
    "id": "BMvT0B-MGBnY"
   },
   "source": [
    "### Baseline: character-level tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "g2QiF-otFur3",
   "metadata": {
    "id": "g2QiF-otFur3"
   },
   "outputs": [],
   "source": [
    "class character_level_tokenizer:\n",
    "    \"\"\"\n",
    "    character-level\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab = [str(x) for x in range(10)] + [\"+\", \"=\"] + [pad_token, eos_token]\n",
    "        self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n",
    "        self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n",
    "        self.ntokens = len(self.vocab)\n",
    "        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n",
    "    \n",
    "    def clean(self, text):\n",
    "        \"\"\"\n",
    "        removes all characters not in the vocabulary\n",
    "        \"\"\"\n",
    "        out = re.sub(self.pattern, \"\", text)\n",
    "        return out\n",
    "\n",
    "    def pre_tokenization(self, text):\n",
    "        \"\"\"\n",
    "        character-level\n",
    "        \"\"\"\n",
    "        return [c for c in text]\n",
    "\n",
    "    def encode(self, text):\n",
    "        text_list = self.pre_tokenization(self.clean(text))\n",
    "        return [self.token_to_id[c] for c in text_list]\n",
    "\n",
    "    def decode(self, token_list):\n",
    "        return \"\".join([self.id_to_token[x] for x in token_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "QuCc6jF5F8hK",
   "metadata": {
    "id": "QuCc6jF5F8hK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = character_level_tokenizer()\n",
    "ntokens = tokenizer.ntokens\n",
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8FXW2K-1Jd-P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8FXW2K-1Jd-P",
    "outputId": "349a4033-9fce-462b-f0d5-1bb3a7ffd340"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 10, 4, 2, 11], '12+42=')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"12 + 42 =\"\n",
    "inputs = tokenizer.encode(prompt)\n",
    "inputs, tokenizer.decode(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j3gckvebGGYt",
   "metadata": {
    "id": "j3gckvebGGYt"
   },
   "source": [
    "# Implement your tokenizer here!\n",
    "\n",
    "You can do anything (as long as you do not compute the addition!).\n",
    "Some ideas:\n",
    "* reversing numbers left to right\n",
    "* arranging by groups (of, 2, 3,...)\n",
    "* aligning numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018453c",
   "metadata": {},
   "source": [
    "## My tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12df70dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "   def __init__(self):\n",
    "       # Initialize vocabulary with digits 0-9\n",
    "       self.vocab = [str(x) for x in range(10)]\n",
    "       # Extend vocabulary with all digit pairs, plus special tokens\n",
    "       self.vocab += [str(item) for item in list(itertools.product(self.vocab, self.vocab))] + [\"+\", \"=\"] + [pad_token, eos_token]\n",
    "\n",
    "       self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n",
    "       self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n",
    "\n",
    "       # Regex patterns for tokenization\n",
    "       self.pattern = re.compile(r'(\\d+|\\[PAD\\]|\\[EOS\\]|\\D)')\n",
    "       self.num_pattern = re.compile(r'\\d+')\n",
    "       self.ntokens = len(self.vocab)\n",
    "\n",
    "   def encode(self, text):\n",
    "       # Remove spaces and split text into components\n",
    "       text = text.replace(\" \", \"\")\n",
    "       text_list = re.findall(self.pattern, text)\n",
    "\n",
    "       new_list = []\n",
    "       # Handle equations of the form (num1+num2)\n",
    "       if len(text_list) > 2 and self.num_pattern.match(text_list[0]) and self.num_pattern.match(text_list[2]):\n",
    "           # Extract and pad numbers to equal length, then reverse\n",
    "           num1 = text_list[0].zfill(max(len(text_list[0]), len(text_list[2])))[::-1]\n",
    "           num2 = text_list[2].zfill(max(len(text_list[0]), len(text_list[2])))[::-1]\n",
    "\n",
    "           # Create digit pairs from aligned numbers\n",
    "           pairs = [str((num1[i], num2[i])) for i in range(len(num1))]\n",
    "           new_list.extend(pairs)\n",
    "\n",
    "           # Process any remaining numbers by reversing and splitting into digits\n",
    "           for i in range(3, len(text_list)):\n",
    "               if self.num_pattern.match(text_list[i]):\n",
    "                   new_list.extend(text_list[i][::-1])\n",
    "               else:\n",
    "                   new_list.append(text_list[i])\n",
    "       else:\n",
    "           # Default case: process each element individually\n",
    "           for elem in text_list:\n",
    "               if self.num_pattern.match(elem):\n",
    "                   # Reverse numbers and split into individual digits\n",
    "                   new_list.extend(elem[::-1])\n",
    "               else:\n",
    "                   new_list.append(elem)\n",
    "\n",
    "       # Convert tokens to IDs\n",
    "       return [self.token_to_id[c] for c in new_list]\n",
    "\n",
    "   def decode(self, token_list):\n",
    "       # Convert IDs back to tokens\n",
    "       tokens = [self.id_to_token[x] for x in token_list if x in self.id_to_token]\n",
    "       \n",
    "       # Case 1: Handle simple reversed digits\n",
    "       if all(t.isdigit() for t in tokens):\n",
    "           return ''.join(tokens[::-1])\n",
    "       \n",
    "       # Case 2: Handle equation with digit pairs\n",
    "       tuples = [t for t in tokens if t.startswith('(') and t.endswith(')')]\n",
    "       if tuples:\n",
    "           # Extract and process digit pairs\n",
    "           pairs = []\n",
    "           for t in tuples:\n",
    "               clean = t.strip('()').replace(\"'\", \"\").replace('\"', \"\")\n",
    "               parts = clean.split(',')\n",
    "               if len(parts) == 2 and parts[0].strip().isdigit() and parts[1].strip().isdigit():\n",
    "                   pairs.append((parts[0].strip(), parts[1].strip()))\n",
    "           \n",
    "           # Reconstruct numbers from pairs and build equation\n",
    "           if pairs:\n",
    "               num1 = ''.join(d[0] for d in pairs)[::-1].lstrip('0') or '0'\n",
    "               num2 = ''.join(d[1] for d in pairs)[::-1].lstrip('0') or '0'\n",
    "               \n",
    "               # Add result if equation has equals sign\n",
    "               if '=' in tokens:\n",
    "                   eq_idx = tokens.index('=')\n",
    "                   result = ''.join(t for t in tokens[eq_idx+1:] if t.isdigit())\n",
    "                   return f\"{num1}+{num2}={result[::-1]}\"\n",
    "               return f\"{num1}+{num2}\"\n",
    "       \n",
    "       # Final fallback, join all tokens\n",
    "       return ''.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8cab53d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = MyTokenizer()\n",
    "ntokens = tokenizer.ntokens\n",
    "ntokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491af297",
   "metadata": {
    "id": "491af297"
   },
   "source": [
    "## Step 2: Create a dataset for arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "daa90f31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "daa90f31",
    "outputId": "3e8719ee-d8fa-4984-8b51-4db3457f7dbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('983+442=', '1425')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_datapoint(number_bits = 3):\n",
    "    \"\"\"\n",
    "    returns a string containing two random numbers on `number_bits` many bits and their sum.\n",
    "    \"\"\"\n",
    "    a_list = [random.randint(0, 9) for _ in range(number_bits)]\n",
    "    b_list = [random.randint(0, 9) for _ in range(number_bits)]\n",
    "    a_int = int(\"\".join([str(x) for x in a_list]))\n",
    "    b_int = int(\"\".join([str(x) for x in b_list]))\n",
    "    sum_int = a_int + b_int\n",
    "    return (str(a_int) + \"+\" + str(b_int) + \"=\", str(sum_int))\n",
    "\n",
    "sample_datapoint(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6e861d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6e861d2",
    "outputId": "c88c2226-0546-473c-c296-88a52823886b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('124441398717+821229841729=', '945671240446'),\n",
       " ('937299185907+993516402866=', '1930815588773'),\n",
       " ('946922115189+566800611371=', '1513722726560'),\n",
       " ('487533766643+194442577408=', '681976344051')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for _ in range(dataset_size):\n",
    "    data.append(sample_datapoint(number_bits))\n",
    "data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fee85050",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fee85050",
    "outputId": "f080f4b0-fd76-48d8-d59f-7c118b6e6fe9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57600, 6400)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = data[: int(train_proportion * dataset_size)]\n",
    "data_test = data[int(train_proportion * dataset_size):]\n",
    "\n",
    "len(data_train),len(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37200598",
   "metadata": {
    "id": "37200598"
   },
   "source": [
    "## Step 3: Construct a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd7d2eb",
   "metadata": {},
   "source": [
    "### Basline: the classical Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91674239",
   "metadata": {
    "id": "91674239"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
    "        Here, we use sine and cosine functions of different frequencies.\n",
    "    .. math:\n",
    "        \\text{PosEmbedder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEmbedder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296ceb2",
   "metadata": {},
   "source": [
    "# Implement your positional embedding here!\n",
    "\n",
    "You can do anything. Some ideas:\n",
    "* RoPE\n",
    "* (randomised) FIRE\n",
    "* Abacus\n",
    "\n",
    "**!!! IMPORTANT !!!** This model of Transformers is \"input first\", meaning that an input is a tensor with shape\n",
    "(length_prompts, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af8790f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dbb20c",
   "metadata": {},
   "source": [
    "## My positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a2dd02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbacusPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1000):\n",
    "        super(AbacusPositionalEmbedding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create empty positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Simple sinusoidal encoding focused on digit positions\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Prepare for addition to input embeddings\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to input embedding\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4eb278ab",
   "metadata": {
    "id": "4eb278ab"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Transformer):\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__(d_model=ninp,\n",
    "                                               nhead=nhead,\n",
    "                                               dim_feedforward=nhid,\n",
    "                                               num_encoder_layers=nlayers)\n",
    "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
    "        self.pos_encoder = AbacusPositionalEmbedding(ninp)\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.ninp = ninp\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        return torch.log(torch.tril(torch.ones(sz,sz)))\n",
    "\n",
    "    def forward(self, src):\n",
    "        mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "        self.src_mask = mask\n",
    "\n",
    "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output_enc = self.encoder(src, mask=self.src_mask)\n",
    "        output_dec = self.decoder(output_enc)\n",
    "        return F.log_softmax(output_dec, dim=-1), output_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30e093a",
   "metadata": {},
   "source": [
    "Please do not change these parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d568cc4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d568cc4",
    "outputId": "f7f78975-2bdf-4c36-de35-3e140636d476"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Linear(in_features=128, out_features=114, bias=True)\n",
       "  (input_emb): Embedding(114, 128)\n",
       "  (pos_encoder): AbacusPositionalEmbedding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerModel(ntoken = ntokens,\n",
    "                         ninp = 128,\n",
    "                         nhead = 16,\n",
    "                         nhid = 64,\n",
    "                         nlayers = 8)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f2f06e0",
   "metadata": {
    "id": "8f2f06e0"
   },
   "outputs": [],
   "source": [
    "def generate(model, prompts, new_tokens = 5):\n",
    "    input_tensor = prompts # (length_prompts, batch_size)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    for _ in range(new_tokens):\n",
    "        output, _ = model(input_tensor) # (length_prompts, batch_size, ntokens)\n",
    "        last_output = output[-1,:,:] # (batch_size, ntokens)\n",
    "        token = torch.argmax(last_output, -1).view((1,-1)) # (1, batch_size)\n",
    "        input_tensor = torch.cat((input_tensor, token), 0)\n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d76d1b19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d76d1b19",
    "outputId": "a1df1dc9-2ecc-4de4-85b2-6bc5bd460439"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 33, 111,  68,   0,   0,   0,   0]], device='mps:0'), '52+83=0000')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "prompt = \"2+3=\"\n",
    "prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
    "output = generate(model, prompt_tensor).view((1,-1))\n",
    "output, tokenizer.decode(output.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "00954ddc",
   "metadata": {
    "id": "00954ddc"
   },
   "outputs": [],
   "source": [
    "def pad(token_list, type_list = \"prompts\"):\n",
    "    max_length = max([len(x) for x in token_list])\n",
    "    out = []\n",
    "    for x in token_list:\n",
    "        if type_list == \"prompts\":\n",
    "            out.append([tokenizer.token_to_id[pad_token]] * (max_length - len(x)) + x)\n",
    "        if type_list == \"answers\":\n",
    "            out.append(x + [tokenizer.token_to_id[eos_token]] + [tokenizer.token_to_id[pad_token]] * (max_length - len(x)))\n",
    "    return out, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2c84beab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c84beab",
    "outputId": "fc1bea13-d6e1-4a55-b70d-36de00bcec9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['1+1=', '21+35='], ['2[EOS][PAD]', '65[EOS]'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [tokenizer.encode(\"1+1=\"), tokenizer.encode(\"21+35=\")]\n",
    "answers = [tokenizer.encode(\"2\"), tokenizer.encode(\"56\")]\n",
    "padded_prompts, _ = pad(prompts, \"prompts\")\n",
    "padded_answers, _ = pad(answers, \"answers\")\n",
    "padded_prompts, padded_answers\n",
    "[tokenizer.decode(p) for p in padded_prompts], [tokenizer.decode(p) for p in padded_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "264f9227",
   "metadata": {
    "id": "264f9227"
   },
   "outputs": [],
   "source": [
    "def get_batch(split, i):\n",
    "    data = data_train if split == 'train' else data_test\n",
    "    prompts = [tokenizer.encode(data[i][0]) for i in range(i, i + batch_size)]\n",
    "    padded_prompts, length_prompts = pad(prompts, \"prompts\")\n",
    "    answers = [tokenizer.encode(data[i][1]) for i in range(i, i + batch_size)]\n",
    "    padded_answers, length_answers = pad(answers, \"answers\")\n",
    "    X = torch.stack([torch.tensor(x) for x in padded_prompts], 1)\n",
    "    Y = torch.stack([torch.tensor(x) for x in padded_answers], 1)\n",
    "    return X, Y, length_prompts, length_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "91e281ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91e281ad",
    "outputId": "22e2d0ee-ede4-41f8-e089-fb63ac2d9787"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13, 64]), torch.Size([14, 64]), 13, 13)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y, length_prompts, length_answers = get_batch(\"train\", 243)\n",
    "X.shape, Y.shape, length_prompts, length_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e1fd1",
   "metadata": {
    "id": "113e1fd1"
   },
   "source": [
    "## Step 4: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1cfcd10a",
   "metadata": {
    "id": "1cfcd10a"
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    # Turn on evaluation mode disables dropout.\n",
    "    model.eval()\n",
    "    correct = 0.\n",
    "    with torch.no_grad():\n",
    "        for batch, i in enumerate(range(0, len(data_test) - 1, batch_size)):\n",
    "            prompts, target_answers, length_prompts, length_answers = get_batch(\"test\", i)\n",
    "            prompts = prompts.to(device) # (length_prompts, batch_size)\n",
    "            target_answers = target_answers.to(device) # (length_answers + 1, batch_size)\n",
    "            output = generate(model, prompts, length_answers + 1) # (length_prompts + length_answers + 1, batch_size)\n",
    "            answers_tokens = output[length_prompts:, :] # (length_answers + 1, batch_size), contains tokens\n",
    "            equality_test = answers_tokens == target_answers # (length_answers + 1, batch_size), contains boolean values\n",
    "            correct += torch.all(equality_test, axis=0).float().sum()\n",
    "        accuracy = correct / len(data_test)\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac335b05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac335b05",
    "outputId": "b475e943-51b3-401d-d18b-c9d32a49ffb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54061a",
   "metadata": {
    "id": "4c54061a"
   },
   "source": [
    "## Step 4: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3638a75d",
   "metadata": {
    "id": "3638a75d"
   },
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
    "        prompts, target_answers, length_prompts, length_answers = get_batch(\"train\", i)\n",
    "        prompts = prompts.to(device) # (length_prompts, batch_size)\n",
    "        target_answers = target_answers.to(device) # (length_answers, batch_size)\n",
    "        input_tensor = torch.cat((prompts, target_answers), 0) # (length_prompts + length_answers, batch_size)\n",
    "        model.zero_grad()\n",
    "        output, _ = model(input_tensor) # (length_prompts + length_answers, batch_size, ntokens)\n",
    "        output_answers = output[length_prompts-1:-1,:,:].reshape(-1, ntokens) # (length_answers * batch_size, ntokens)\n",
    "        target_answers = target_answers.view(-1)\n",
    "        loss = F.cross_entropy(output_answers, target_answers)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f} | perplexity {:8.2f}'.format(batch, len(data_train) // batch_size,\n",
    "                                                                                                        elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def train():\n",
    "    best_test_accuracy = None\n",
    "    test_accuracy = evaluate()\n",
    "    print('-' * 89)\n",
    "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
    "    print('-' * 89)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_epoch()\n",
    "        test_accuracy = evaluate()\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the test accuracy is the best we've seen so far.\n",
    "        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
    "            with open(\"arithmetic.pt\", 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_test_accuracy = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4e2a8490",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e2a8490",
    "outputId": "f70dcac2-5891-4266-8748-85df050f4881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| initialisation | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 57.34 | loss  2.17 | perplexity     8.80\n",
      "|   400/  900 batches | ms/batch 58.51 | loss  1.22 | perplexity     3.39\n",
      "|   600/  900 batches | ms/batch 58.54 | loss  0.17 | perplexity     1.19\n",
      "|   800/  900 batches | ms/batch 58.09 | loss  0.05 | perplexity     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 77.16s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 60.82 | loss  0.03 | perplexity     1.03\n",
      "|   400/  900 batches | ms/batch 56.88 | loss  0.03 | perplexity     1.03\n",
      "|   600/  900 batches | ms/batch 60.48 | loss  0.02 | perplexity     1.02\n",
      "|   800/  900 batches | ms/batch 60.57 | loss  0.02 | perplexity     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 81.39s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 60.31 | loss  0.02 | perplexity     1.02\n",
      "|   400/  900 batches | ms/batch 61.35 | loss  0.02 | perplexity     1.02\n",
      "|   600/  900 batches | ms/batch 61.73 | loss  0.02 | perplexity     1.02\n",
      "|   800/  900 batches | ms/batch 63.30 | loss  0.02 | perplexity     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 81.32s | test accuracy  1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 67.86 | loss  0.01 | perplexity     1.01\n",
      "|   400/  900 batches | ms/batch 67.91 | loss  0.02 | perplexity     1.02\n",
      "|   600/  900 batches | ms/batch 70.43 | loss  0.01 | perplexity     1.01\n",
      "|   800/  900 batches | ms/batch 69.77 | loss  0.01 | perplexity     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 89.89s | test accuracy  1.00\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "56d9d440",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56d9d440",
    "outputId": "1872232b-b120-440b-e1a6-666e079efa3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "996106597082+445195073285=1441301670367\t actual result: 1441301670367\n",
      "172902337353+543860561905=716762899258\t actual result: 716762899258\n",
      "876030450100+234838827760=1110869277860\t actual result: 1110869277860\n",
      "153046479547+90632912264=243679391811\t actual result: 243679391811\n",
      "114750700752+976000521104=1090751221856\t actual result: 1090751221856\n",
      "521931289912+792045587590=1313976877502\t actual result: 1313976877502\n",
      "920879056650+707814711732=1628693768382\t actual result: 1628693768382\n",
      "573709497379+250513637183=824223134562\t actual result: 824223134562\n",
      "673054406245+325047874160=998102280405\t actual result: 998102280405\n",
      "49797655384+378807193437=428604848821\t actual result: 428604848821\n",
      "373562768775+382237869698=755800638473\t actual result: 755800638473\n",
      "387972753517+961995164473=1349967917990\t actual result: 1349967917990\n",
      "328316127883+765309242356=1093625370239\t actual result: 1093625370239\n",
      "84728028247+159508358630=244236386877\t actual result: 244236386877\n",
      "779760423185+89951121052=869711544237\t actual result: 869711544237\n",
      "387385513906+545138404678=932523918584\t actual result: 932523918584\n",
      "347600583277+355724944683=703325527960\t actual result: 703325527960\n",
      "820129599080+474057074944=1294186674024\t actual result: 1294186674024\n",
      "816261259727+296927704937=1113188964664\t actual result: 1113188964664\n",
      "743307171048+505171638385=1248478809433\t actual result: 1248478809433\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "for i in range(20):\n",
    "    prompt, answers = data_test[i]\n",
    "    prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
    "    output = generate(model, prompt_tensor, len(answers)).view((1,-1))\n",
    "    print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c6da0479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('996106597082+445195073285=', '1441301670367'),\n",
       " ('172902337353+543860561905=', '716762899258'),\n",
       " ('876030450100+234838827760=', '1110869277860'),\n",
       " ('153046479547+90632912264=', '243679391811'),\n",
       " ('114750700752+976000521104=', '1090751221856'),\n",
       " ('521931289912+792045587590=', '1313976877502'),\n",
       " ('920879056650+707814711732=', '1628693768382'),\n",
       " ('573709497379+250513637183=', '824223134562'),\n",
       " ('673054406245+325047874160=', '998102280405'),\n",
       " ('49797655384+378807193437=', '428604848821'),\n",
       " ('373562768775+382237869698=', '755800638473'),\n",
       " ('387972753517+961995164473=', '1349967917990'),\n",
       " ('328316127883+765309242356=', '1093625370239'),\n",
       " ('84728028247+159508358630=', '244236386877'),\n",
       " ('779760423185+89951121052=', '869711544237'),\n",
       " ('387385513906+545138404678=', '932523918584'),\n",
       " ('347600583277+355724944683=', '703325527960'),\n",
       " ('820129599080+474057074944=', '1294186674024'),\n",
       " ('816261259727+296927704937=', '1113188964664'),\n",
       " ('743307171048+505171638385=', '1248478809433')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qJ9IOZu8Xo4Y",
   "metadata": {
    "id": "qJ9IOZu8Xo4Y"
   },
   "source": [
    "## Probing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78be1213",
   "metadata": {},
   "source": [
    "This is just for fun..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "yomPfirhXkLb",
   "metadata": {
    "id": "yomPfirhXkLb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_size = 1000\n",
    "test_size = 100\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def data_probing(size):\n",
    "    X = []\n",
    "    y = np.zeros(size)\n",
    "    for i in range(size):\n",
    "        input = torch.tensor(tokenizer.encode(data[i][0])).view((-1, 1)).to(device)\n",
    "        _, output = model(input)\n",
    "        output = output[-1,:,:].flatten()\n",
    "        # determine whether there was a carry in the result:\n",
    "        carry = len(data[i][1]) > len(data[i][0]) / 2\n",
    "        X.append(output.cpu().detach().numpy())\n",
    "        y[i] = carry\n",
    "    return np.array(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "QGmfXVxkppfP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGmfXVxkppfP",
    "outputId": "6601c884-004f-40bb-8a1a-71995b17d860"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, y_train = data_probing(train_size)\n",
    "X_test, y_test = data_probing(test_size)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "reg = LogisticRegression()\n",
    "reg.fit(X_train,y_train)\n",
    "reg.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
