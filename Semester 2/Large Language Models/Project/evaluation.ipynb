{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation quantized models (pruned/no-pruned versions)"
      ],
      "metadata": {
        "id": "2tEw-X4nqM-I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8sWzkgSGbIyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6aecfef-bc28-4869-8d8b-107355f4fb5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -U transformers bitsandbytes accelerate lm-eval datasets jinja2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_CKxvtGqbWc0"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers.testing_utils import CaptureLogger\n",
        "from transformers.models.mixtral.modeling_mixtral import (\n",
        "    MixtralForCausalLM,\n",
        "    MixtralSparseMoeBlock,\n",
        "    MixtralBlockSparseTop2MLP\n",
        "    )\n",
        "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, set_seed\n",
        "import torch\n",
        "import bitsandbytes as bnb\n",
        "from lm_eval.api.model import LM\n",
        "from lm_eval.models.huggingface import HFLM\n",
        "\n",
        "from lm_eval import evaluator\n",
        "from jinja2 import Template\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HpLgSJIZbt8X"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okd8zS-sq8wk",
        "outputId": "a5d2da15-99d9-4514-996f-18ebadd83485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  results_dir = \"/content/drive/MyDrive/MVA/LLM/results\"\n",
        "  # Create it if does not exist\n",
        "  if not os.path.exists(results_dir):\n",
        "    !mkdir -p $results_dir\n",
        "else:\n",
        "  results_dir = \"./results\"\n",
        "  if not os.path.exists(results_dir):\n",
        "    os.mkdir(results_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pga6alNNsOUC"
      },
      "source": [
        "# 1. Testing inference in quantized - pruned Mixtral-8x7B-Instruct-v.0.1\n",
        "\n",
        "A few examples about generation text with pruned version of Mixtral-8x7B-Instruct-v.0.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ACK_lBlbrlK"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"JavierLopetegui/Mixtral8x7B-4bit-pruned_4_experts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQEaUwZgQEqM",
        "outputId": "66f009e9-849a-4997-98e0-512e4c421a48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MixtralForCausalLM(\n",
              "  (model): MixtralModel(\n",
              "    (embed_tokens): Embedding(32000, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x MixtralDecoderLayer(\n",
              "        (self_attn): MixtralAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
              "          (gate): Linear4bit(in_features=4096, out_features=4, bias=False)\n",
              "          (experts): ModuleList(\n",
              "            (0-3): 4 x MixtralBlockSparseTop2MLP(\n",
              "              (w1): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (w2): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "              (w3): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (input_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): MixtralRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): MixtralRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T97nYJb3Rr8H"
      },
      "outputs": [],
      "source": [
        "def print_instance_params(instance):\n",
        "    for attr in [\n",
        "        \"blocksize\",\n",
        "        \"compress_statistics\",\n",
        "        \"quant_type\",\n",
        "        \"quant_state\",\n",
        "        \"quant_storage\",\n",
        "        \"bnb_quantized\",\n",
        "        \"data\",\n",
        "        \"module\",\n",
        "    ]:\n",
        "        print(f\"{attr}: {getattr(instance, attr, 'Not Found')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TiC2Pl2Sd9e"
      },
      "outputs": [],
      "source": [
        "def print_instance_params_v2(instance):\n",
        "    for attr in [\n",
        "        \"weight\",\n",
        "        \"compute_dtype\",\n",
        "        \"compute_type_is_set\",\n",
        "        \"quant_state\",\n",
        "        \"quant_storage\",\n",
        "    ]:\n",
        "        print(f\"{attr}: {getattr(instance, attr, 'Not Found')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTBH5DCpqCuq"
      },
      "outputs": [],
      "source": [
        "def generate_text_given_prompt(model,tokenizer, prompt, max_new_tokens=500, temperature=0.7, num_return_sequences=1, repetition_penalty=1.0):\n",
        "    if isinstance(prompt, list):\n",
        "      prompt = tokenizer.apply_chat_template(prompt, tokenize=False, return_tensors=\"pt\")\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids.to('cuda')\n",
        "    attention_mask = inputs.attention_mask.to('cuda')\n",
        "    prompt_length = input_ids.shape[-1]\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            attention_mask=attention_mask,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "        )\n",
        "    '''\n",
        "    if isinstance(prompt, list):\n",
        "      inputs = tokenizer.apply_chat_template(prompt, return_tensors=\"pt\").to('cuda')\n",
        "      prompt_length = inputs.shape[-1]\n",
        "      with torch.inference_mode():\n",
        "          output = model.generate(\n",
        "              inputs,\n",
        "              max_new_tokens=max_new_tokens,\n",
        "              temperature=temperature,\n",
        "              do_sample=True,\n",
        "              num_return_sequences=num_return_sequences,\n",
        "              pad_token_id=tokenizer.eos_token_id,\n",
        "              eos_token_id=tokenizer.eos_token_id,\n",
        "              repetition_penalty=repetition_penalty,\n",
        "          )\n",
        "\n",
        "    else:\n",
        "      inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "      input_ids = inputs.input_ids.to('cuda')\n",
        "      attention_mask = inputs.attention_mask.to('cuda')\n",
        "      prompt_length = input_ids.shape[-1]\n",
        "\n",
        "      with torch.inference_mode():\n",
        "          output = model.generate(\n",
        "              input_ids,\n",
        "              max_new_tokens=max_new_tokens,\n",
        "              attention_mask=attention_mask,\n",
        "              temperature=temperature,\n",
        "              do_sample=True,\n",
        "              num_return_sequences=num_return_sequences,\n",
        "              pad_token_id=tokenizer.eos_token_id,\n",
        "              eos_token_id=tokenizer.eos_token_id,\n",
        "              repetition_penalty=repetition_penalty,\n",
        "          )\n",
        "    '''\n",
        "\n",
        "    # Process each sequence and remove the prompt part\n",
        "    generated_texts = []\n",
        "    for sequence in output:\n",
        "        generated_part = sequence[prompt_length:]  # Remove prompt\n",
        "        text = tokenizer.decode(generated_part, skip_special_tokens=True)\n",
        "        generated_texts.append(text)\n",
        "\n",
        "    return generated_texts if num_return_sequences > 1 else generated_texts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wEGoi3rqTuw"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"Complete the story: Once upon a time, my mother taught me how to make a simple loaf of bread\",\n",
        "    \"Give me the result of the next operation: 15 + 25\",\n",
        "    \"Brief summary of France history:\",\n",
        "    \"The result of multiplying 13 by 3 is: \",\n",
        "    \"Once upon a time,\"\n",
        "    \"Yo voy todos los días a la escuela se traduce a frances como: \"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QhXcCBIwqgEs",
        "outputId": "d4092968-45f1-430a-8a55-fb14e6d44728"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Given prompt: \n",
            "\tComplete the story: Once upon a time, my mother taught me how to make a simple loaf of bread\n",
            "\n",
            "\n",
            "Generated answer: \n",
            "\t. This recipe was passed to me by my grandmother. At that time, I was too young to know what importance laid in that small piece of bread. Little did I know that one day I might have to rely on this recipe to feed my family. Now I am elderly, and this recipe is my only treasure. I am grateful that my mother and grandmother passed this recipe to me. I wish that someday I could pass this recipe to someone I love.\n",
            "\n",
            "This is a great recipe. I’ve only made the loaf but I am sure I will be making the rest. The instructions are easy and clear, the ingredients are easy and available, and the taste is great. I am less than 80 years old, but I hope to have the opportunity to pass this recipe on to someone I love.\n",
            "\n",
            "Ingredients\n",
            "\n",
            "1\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Given prompt: \n",
            "\tGive me the result of the next operation: 15 + 25\n",
            "\n",
            "\n",
            "Generated answer: \n",
            "\t- 35?\n",
            "\n",
            "Answer: 15 + 25 - 35 = -10\n",
            "\n",
            "The next operation is 15 + 25 - 35.\n",
            "\n",
            "Let's start by adding 15 and 25 together.\n",
            "\n",
            "Those numbers add up to 40.\n",
            "\n",
            "Now, let's subtract 35 from 40.\n",
            "\n",
            "We get 40 - 35 = -10.\n",
            "\n",
            "\n",
            "Now we are ready to answer the question.\n",
            "\n",
            "The result of the next operation is -10.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Given prompt: \n",
            "\tBrief summary of France history:\n",
            "\n",
            "\n",
            "Generated answer: \n",
            "\t\n",
            "\n",
            "At the beginning of the 19th century,\n",
            "\n",
            "The Empire of Napoleon was established and ruled by Napoleon Bonaparte.\n",
            "\n",
            "The Empire was the first modern European country to use systematically the decimal system of money, which was based on the metric system.\n",
            "\n",
            "The Empire was noted for the building of many roads, bridges, canals and other types of infrastructure.\n",
            "\n",
            "The Empire was also known for its culture, architecture and fashion.\n",
            "\n",
            "It was also noted for its educational system and high literacy rate.\n",
            "\n",
            "In 1815, the Empire was defeated in the Battle of Waterloo by the British and the Europeans.\n",
            "\n",
            "After the Battle, the Empire was replaced by the Kingdom of France.\n",
            "\n",
            "In 1848, the Kingdom was replaced by the French Republic.\n",
            "\n",
            "The Republic was governed by a President and a legislative assembly.\n",
            "\n",
            "The\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Given prompt: \n",
            "\tThe result of multiplying 13 by 3 is: \n",
            "\n",
            "\n",
            "Generated answer: \n",
            "\t39.\n",
            "The result of multiplying 13 by 4 is: 52.\n",
            "The result of multiplying 13 by 5 is: 65.\n",
            "The result of multiplying 13 by 6 is: 78.\n",
            "The result of multiplying 13 by 7 is: 91.\n",
            "The result of multiplying 13 by 8 is: 104.\n",
            "The result of multiplying 13 by 9 is: 117.\n",
            "The result of multiplying 13 by 10 is: 130.\n",
            "The result of multiplying 13 by 11 is: 143.\n",
            "The result of multiplying 13 by 12 is: 156.\n",
            "The result of multiplying 13 by 1\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Given prompt: \n",
            "\tOnce upon a time,Yo voy todos los días a la escuela se traduce a frances como: \n",
            "\n",
            "\n",
            "Generated answer: \n",
            "\tJe vais à l’école tous les jours.But it's a bit more complicated than that.\n",
            "\n",
            "Before we delve into the complexities of the French language, let’s practice saying “je vais à l’école tous les jours” with a beautiful French pronunciation:\n",
            "\n",
            " [Juh-vah AH-kool-loh-TUHZ-loh-z-HJAY ]\n",
            "\n",
            "Now, let’s examine the words in this sentence:\n",
            "\n",
            "Juh-vah = Je\n",
            "\n",
            "Ah-kool = École\n",
            "\n",
            "TUHZ-loh-z = Tous les jours\n",
            "\n",
            "HJAY = days\n",
            "\n",
            "In this sentence, there is one word that requires more attention than the others: Tous les jours.\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for prompt in prompts:\n",
        "    print(f\"Given prompt: \\n\\t{prompt}\\n\\n\")\n",
        "    print(f\"Generated answer: \\n\\t{generate_text_given_prompt(model, tokenizer, prompt)}\\n\")\n",
        "    print(\"-\"*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_-cAKuQytD1"
      },
      "source": [
        "It can be seen that the model tend to hallucinate or answer extra questions. We try to fix this by generating structured templates as prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vGr1KrNzUtx"
      },
      "outputs": [],
      "source": [
        "# Functions to use templates as normal prompt\n",
        "\n",
        "def chat_messages_to_prompt(messages):\n",
        "    '''\n",
        "    Modify a templete to generate a plain prompt\n",
        "    '''\n",
        "    prompt = \"\"\n",
        "    for msg in messages:\n",
        "        if msg[\"role\"] == \"system\":\n",
        "            prompt += f\"System: {msg['content']}\\n\"\n",
        "        elif msg[\"role\"] == \"user\":\n",
        "            prompt += f\"User: {msg['content']}\\n\"\n",
        "        elif msg[\"role\"] == \"assistant\":\n",
        "            prompt += f\"Assistant: {msg['content']}\\n\"\n",
        "    prompt += \"Assistant: \"  # Signals the model to continue\n",
        "    return prompt\n",
        "\n",
        "def prepare_prompt(prompt_or_messages):\n",
        "    '''\n",
        "    Auxiliar function to modify a list of templates to plain prompts\n",
        "    '''\n",
        "    if isinstance(prompt_or_messages, list):\n",
        "        return chat_messages_to_prompt(prompt_or_messages)\n",
        "    return prompt_or_messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABEKBV1xzbcj"
      },
      "outputs": [],
      "source": [
        "chat_prompts = [\n",
        "    [\n",
        "        {\"role\": \"system\", \"content\": \"You are a creative and imaginative storyteller AI.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Complete the story: Once upon a time, my mother taught me how to make a simple loaf of bread.\"}\n",
        "    ],\n",
        "    [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful math expert. ONLY provide the result of the operation, just one answer.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the result of the following operation: 15 + 25\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The answer is: 40\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the result of the following operation: 15 - 25\"}\n",
        "    ],\n",
        "    [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful math expert. Provide first the explanation, then the result. END after the result. Think step-by-step.\"},\n",
        "        {\"role\": \"user\", \"content\": \"I want you to solve the following mathematical problem. Please, you should be detailed in you answer and do it in logical steps. \\\n",
        "        This is the problem: In how many ways can 5 students be selected from a group of 6 students?\"}\n",
        "    ],\n",
        "    [\n",
        "        {\"role\": \"system\", \"content\": \"You are a knowledgeable historian.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Give me a brief summary of the history of France.\"}\n",
        "    ],\n",
        "    [\n",
        "        {\"role\": \"system\", \"content\": \"You are a math expert. END after the result. Think step-by-step.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the result of multiplying 13 by 3?\"}\n",
        "    ],\n",
        "    [\n",
        "        {\"role\": \"system\", \"content\": \"You are a translator Spanish-to-French expert. ONLY provide the translated sentence in FRENCH. Do NOT add any explanations, or additional content. End after the translation.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Translate the following: Yo voy todos los días a la escuela\"}\n",
        "    ]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_4Ty8yQ-0ZVs",
        "outputId": "4432e7a1-a55d-45c3-8d7a-f8f7cdf15f14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Given prompt: \n",
            "System: You are a creative and imaginative storyteller AI.\n",
            "User: Complete the story: Once upon a time, my mother taught me how to make a simple loaf of bread.\n",
            "\n",
            "\n",
            "\n",
            "Once upon a time, my mother taught me how to make a simple loaf of bread. It was a magical experience, as if I had been transported into a mystical world of baking and feasting. I was only six years old, but my mother believed in my abilities. She guided me through the process, step by step. I remember the warmth of the oven, the smell of the bread, and the pride I had when I pulled out the loaf, all on my own. I knew that I had created something truly special. I was not just a simple loaf of bread, but a piece of art, a product of my creativity, and the beginnings of a master baker.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Given prompt: \n",
            "System: You are a helpful math expert. ONLY provide the result of the operation, just one answer.\n",
            "User: What is the result of the following operation: 15 + 25?\n",
            "Assistant: The answer is: 40\n",
            "User: What is the result of the following operation: 15 - 25?\n",
            "\n",
            "\n",
            "\n",
            "The answer is: -20\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Given prompt: \n",
            "System: You are a helpful math expert. Provide first the explanation, then the result. END after the result. Think step-by-step.\n",
            "User: I want you to solve the following mathematical problem. Please, you should be detailed in you answer and do it in logical steps.         This is the problem: In how many ways can 5 students be selected from a group of 6 students?\n",
            "\n",
            "\n",
            "\n",
            "Here is my step-by-step solution to your problem:\n",
            "\n",
            "First, I need to clarify that there are 6 students in the group, and we are trying to select 5 of them.  \n",
            "\n",
            "There are 6 ways to choose the first student, since they are the only one being chosen from the group. \n",
            "\n",
            "The remaining 4 students out of the 6 form the possible selections for the second student. \n",
            "\n",
            "The remaining 3 students out of the 6 form the possible selections for the third student. \n",
            "\n",
            "The remaining 2 students out of the 6 form the possible selections for the fourth student. \n",
            "\n",
            "There is only one student left, so they are the fifth student. \n",
            "\n",
            "Since there are 6 ways to choose the first student, I need to divide the number of ways to choose the second, third, and fourth students by 6. \n",
            "\n",
            "Therefore, there\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Given prompt: \n",
            "System: You are a knowledgeable historian.\n",
            "User: Give me a brief summary of the history of France.\n",
            "\n",
            "\n",
            "\n",
            "The history of France is vast and complex, but I will provide a brief summary of some of the key events and periods.\n",
            "\n",
            "The earliest recorded history of France dates back to the Gallo-Romans, a Celtic people who lived in the area during the Roman conquest. France was first unified under the Roman Empire, then by the Merovingians, and finally by Charlemagne in the 8th century.\n",
            "\n",
            "During the Middle Ages, France was divided into many smaller feudal states, each ruled by a noble family.\n",
            "\n",
            "The first centralized French state emerged during the reign of Philip the Fair in the 13th century.\n",
            "\n",
            "In the 14th century, the Hundred Years War between France and England resulted in the creation of the first large, centralized nation state in France.\n",
            "\n",
            "During the 15th century, France was united under the rule of Louis XI, who was known as the Father\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Given prompt: \n",
            "System: You are a math expert. END after the result. Think step-by-step.\n",
            "User: What is the result of multiplying 13 by 3?\n",
            "\n",
            "\n",
            "\n",
            "Here's how I would solve this step-by-step: \n",
            "\n",
            "1. Identify the numbers to be multiplied: 13 and 3. \n",
            "\n",
            "2. Memorize the order of the numbers: 13 times 3. \n",
            "\n",
            "3. Remember a multiplication fact: 1 times 3 = 3. \n",
            "\n",
            "4. Recall the process: In a multiplication problem, if you know what one times the second number is, you can think of it as \"a number of times b\". \n",
            "\n",
            "5. Multiply the number you remembered (3) by the number of times (13): \n",
            "\n",
            "6. Repeat the process of recalling a multiplication fact and multiplying: \n",
            "\n",
            "7. Result: 39. \n",
            "\n",
            "Let's remember that the process of multiplication is the same no matter how big the numbers are. You can use this process\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Given prompt: \n",
            "System: You are a translator Spanish-to-French expert. ONLY provide the translated sentence. Do NOT add any explanations, or additional content. End after the translation.\n",
            "User: Translate the following: Yo voy todos los días a la escuela\n",
            "\n",
            "\n",
            "\n",
            "In French, this would be: Je vais à l'école tous les jours.\n",
            "\n",
            "Translation: Je vais à l'école tous les jours.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for prompt in chat_prompts:\n",
        "    print(f\"Given prompt: \\n{prepare_prompt(prompt)}\\n\\n\")\n",
        "    print(f\"{generate_text_given_prompt(model, tokenizer, prompt)}\\n\")\n",
        "    print(\"-\"*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSSopG1TAxxc"
      },
      "source": [
        "As can be seen with this simple examples, using a template with proper prompts improve the quality of the predictions. We will use this kind of prompts to compare the performance between pruned models and no-pruned models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimental pipeline functions\n",
        "\n",
        "Following the same general tasks that have been evaluated in [1] we will use the library [lm-eval](https://github.com/EleutherAI/lm-evaluation-harness/tree/2a47159caff00135b026f724ace2a2011f3c7621). A unified framework to test generative language models on a large number of different evaluation tasks.\n",
        "\n",
        "**Remark:** For compatibility with the library a new class is defined to have tokenizer and model inside the HFLM/LM class from lm-eval."
      ],
      "metadata": {
        "id": "ZtOGGt9pnv7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCustomQuantizedLM(HFLM):\n",
        "    def __init__(self, model, tokenizer, already_quant = False, device=device):\n",
        "        if isinstance(model, str):\n",
        "            if already_quant:\n",
        "              model = AutoModelForCausalLM.from_pretrained(model)\n",
        "            else:\n",
        "              quantization_config = BitsAndBytesConfig(\n",
        "                  load_in_4bit=True,\n",
        "                  bnb_4bit_compute_dtype=torch.float16,\n",
        "                  bnb_4bit_quant_type=\"nf4\",\n",
        "                  bnb_4bit_use_double_quant=True,\n",
        "              )\n",
        "              model = AutoModelForCausalLM.from_pretrained(\n",
        "                                model,\n",
        "                                device_map=device,\n",
        "                                quantization_config=quantization_config\n",
        "                            )\n",
        "\n",
        "        if isinstance(tokenizer, str):\n",
        "            tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
        "\n",
        "        super().__init__(\n",
        "            pretrained=model,\n",
        "            tokenizer=tokenizer,\n",
        "            device=device,\n",
        "            max_length=2048\n",
        "        )\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token"
      ],
      "metadata": {
        "id": "jOUj8dkDn98F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, torch.dtype):\n",
        "            return str(obj)  # Convert torch.dtype to string\n",
        "        if isinstance(obj, torch.device):  # Check if it's a torch.device\n",
        "            return str(obj)  # Convert torch.device to string\n",
        "        return json.JSONEncoder.default(self, obj)"
      ],
      "metadata": {
        "id": "lb-qkol0oD6D"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def performance_evaluation(model, tasks, new_fewshots=0, apply_chat_template=True, results_dir='./', device=device):\n",
        "    '''\n",
        "    Evaluates a generative language model on a list of evaluation tasks using the lm-eval framework.\n",
        "\n",
        "    Args:\n",
        "        model:\n",
        "            The language model to be evaluated. Should be compatible with lm-eval's evaluation API.\n",
        "        tasks (list):\n",
        "            A list of task names (strings) representing evaluation benchmarks or datasets supported by lm-eval.\n",
        "        new_fewshots (int, optional):\n",
        "            Number of few-shot examples to use during evaluation. Default is 0 (zero-shot).\n",
        "        apply_chat_template (bool, optional):\n",
        "            Whether to apply chat formatting templates during evaluation. Useful for chat-based models. Default is True.\n",
        "        results_dir (str, optional):\n",
        "            Directory path where the evaluation results (JSON) will be saved. Default is './'.\n",
        "        device (str or torch.device):\n",
        "            The device on which the model should be evaluated (e.g., 'cuda', 'cpu').\n",
        "\n",
        "    Output:\n",
        "        JSON result files named \"<task>_eval_results.json\" saved in the results directory.\n",
        "    '''\n",
        "    for task in tasks:\n",
        "      # Run evaluation (simulate here; real dataset will use task definition)\n",
        "      results = evaluator.simple_evaluate(\n",
        "          model=model,\n",
        "          tasks=[task],\n",
        "          num_fewshot=new_fewshots,\n",
        "          device=device,\n",
        "          apply_chat_template=apply_chat_template\n",
        "      )\n",
        "\n",
        "      # Make dire if doesn't exist\n",
        "      if not os.path.exists(results_dir):\n",
        "        os.mkdir(results_dir)\n",
        "      # Save or print result\n",
        "      with open(f\"{results_dir}/{task}_eval_results.json\", \"w\") as f:\n",
        "          json.dump(results, f, indent=2, cls=CustomEncoder)\n",
        "\n",
        "      print(f\" Evaluation complete for {task}. Results saved.\\n\")"
      ],
      "metadata": {
        "id": "tIGCu7l9oE97"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LSXU-O1GqyZ"
      },
      "source": [
        "# 3. Evaluation: Mixtral-Pruned (r=4)\n",
        "\n",
        "**Remark:** Due to computational limitations we jsut run the inference evaluation on `[\"arc_challenge\",\"arc_easy\", \"boolq\", \"openbookqa\",\"rte\"]` datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGfacPBrfZdi"
      },
      "outputs": [],
      "source": [
        "# Uncomment if not loaded before\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"JavierLopetegui/Mixtral8x7B-4bit-pruned_4_experts\")\n",
        "\n",
        "# Instantate model\n",
        "quantized_lm = MyCustomQuantizedLM(model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "performance_evaluation(model=quantized_lm,\n",
        "                       tasks=[\"arc_challenge\",\"arc_easy\", \"boolq\", \"openbookqa\",\"rte\"],\n",
        "                       new_fewshots=0,\n",
        "                       apply_chat_template=True,\n",
        "                       results_dir=f\"{results_dir}/mixtral-pruned-r4\",\n",
        "                       device=device)"
      ],
      "metadata": {
        "id": "Hlr9FL9ZoyNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Evaluation: Mixtral-Pruned (r=6)"
      ],
      "metadata": {
        "id": "SG2HCrIMdQg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"JavierLopetegui/Mixtral8x7B-4bit-pruned\")\n",
        "\n",
        "quantized_lm = MyCustomQuantizedLM(model, tokenizer)"
      ],
      "metadata": {
        "id": "h1_kuWiHdQ3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_evaluation(model=quantized_lm,\n",
        "                       tasks=[\"arc_challenge\",\"arc_easy\", \"boolq\", \"openbookqa\",\"rte\"],\n",
        "                       new_fewshots=0,\n",
        "                       apply_chat_template=True,\n",
        "                       results_dir=f\"{results_dir}/mixtral-pruned-r6\",\n",
        "                       device=device)"
      ],
      "metadata": {
        "id": "skTC_G7DshtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOA1R0iNuIO0"
      },
      "source": [
        "# Evaluation: Mixtral no-pruned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXUN7slAS2Rr"
      },
      "outputs": [],
      "source": [
        "# Load model quantized in 4-bit\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "    device_map='cuda',\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "\n",
        "quantized_lm = MyCustomQuantizedLM(model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "performance_evaluation(model=quantized_lm,\n",
        "                       tasks=[\"arc_challenge\",\"arc_easy\", \"boolq\", \"openbookqa\",\"rte\"],\n",
        "                       new_fewshots=0,\n",
        "                       apply_chat_template=True,\n",
        "                       results_dir=f\"{results_dir}/mixtral-no-pruned\",\n",
        "                       device=device)"
      ],
      "metadata": {
        "id": "TvadT-DwstIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation: DeepSeek pruned"
      ],
      "metadata": {
        "id": "1aE1tWxX_QzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"olijacklu/deepseek-moe-16b-pruned\", trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"olijacklu/deepseek-moe-16b-pruned\", trust_remote_code=True)\n",
        "\n",
        "quantized_lm = MyCustomQuantizedLM(model, tokenizer)"
      ],
      "metadata": {
        "id": "SuQSMJKB_QYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_evaluation(model=quantized_lm,\n",
        "                       tasks=[\"arc_challenge\",\"arc_easy\", \"boolq\", \"openbookqa\",\"rte\"],\n",
        "                       new_fewshots=0,\n",
        "                       apply_chat_template=False,\n",
        "                       results_dir=f\"{results_dir}/deepseek-pruned\",\n",
        "                       device=device)"
      ],
      "metadata": {
        "id": "k0HqXTx7AlpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation: DeepSeek no-pruned"
      ],
      "metadata": {
        "id": "c6nlFRcJBbnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-moe-16b-base\", trust_remote_code=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"deepseek-ai/deepseek-moe-16b-base\",\n",
        "    device_map='cuda',\n",
        "    quantization_config=quantization_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "quantized_lm = MyCustomQuantizedLM(model, tokenizer)"
      ],
      "metadata": {
        "id": "VN6qFHehBbYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_evaluation(model=quantized_lm,\n",
        "                       tasks=[\"arc_challenge\",\"arc_easy\", \"boolq\", \"openbookqa\",\"rte\"],\n",
        "                       new_fewshots=0,\n",
        "                       apply_chat_template=False,\n",
        "                       results_dir=f\"{results_dir}/deepseek-no-pruned\",\n",
        "                       device=device)"
      ],
      "metadata": {
        "id": "rF0cjLzfl4l_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}