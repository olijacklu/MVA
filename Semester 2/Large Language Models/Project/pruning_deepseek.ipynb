{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DeepSeek MoE 16B Base Model Pruning (from 64 to 16 experts per layer)"
      ],
      "metadata": {
        "id": "JeofT_xFxwfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive (if using Google Colab)"
      ],
      "metadata": {
        "id": "JDIWr3P9x6xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FzIIpyL2SaFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install necessary packages"
      ],
      "metadata": {
        "id": "zd7IIBB1yAFC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_EBiTdkSPdV"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "wKud30R1yGW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, set_seed\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "from data import CacheDataset, build_calib_loader_deepseek\n",
        "from model import PrunableDeepseekMoEWrapper\n",
        "from method import progressive_pruning_deepseek\n",
        "\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "id": "irLuV0f1SYc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set base directory for saving the model (e.g. your current working directory)"
      ],
      "metadata": {
        "id": "h04a3PGcyJBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/drive/MyDrive/MVA/LLM\""
      ],
      "metadata": {
        "id": "kIJSb7dJxnFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main execution with 4-bit quantization of DeepSeek MoE 16B Base model"
      ],
      "metadata": {
        "id": "vI5K9D6P1Tfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "   set_seed(42)\n",
        "\n",
        "   model_name = \"deepseek-ai/deepseek-moe-16b-base\"\n",
        "\n",
        "   quantization_config = BitsAndBytesConfig(\n",
        "       load_in_4bit=True,\n",
        "       bnb_4bit_compute_dtype=torch.float16,\n",
        "       bnb_4bit_quant_type=\"nf4\",\n",
        "       bnb_4bit_use_double_quant=True\n",
        "   )\n",
        "\n",
        "   tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "   model = AutoModelForCausalLM.from_pretrained(\n",
        "       model_name,\n",
        "       device_map='cuda',\n",
        "       quantization_config=quantization_config,\n",
        "       trust_remote_code=True\n",
        "   )\n",
        "\n",
        "   calib_loader = build_calib_loader_deepseek(\"c4\", tokenizer, 2048, 64, 4, 8, 42)\n",
        "\n",
        "   with torch.no_grad():\n",
        "       model, info = progressive_pruning_deepseek(model, calib_loader, r=16)\n",
        "\n",
        "   model.save_pretrained(os.path.join(base_dir, \"deepseek-moe-16b-pruned\"))\n",
        "   tokenizer.save_pretrained(os.path.join(base_dir, \"deepseek-moe-16b-pruned\"))"
      ],
      "metadata": {
        "id": "p14AMlPlS_bf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}